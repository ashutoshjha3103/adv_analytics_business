{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting price prediction model...\n",
      "Preparing data for final modeling...\n",
      "Encoding categorical features...\n",
      "Scaling features...\n",
      "Training LightGBM models...\n",
      "Training Random Forest model...\n",
      "Training XGBoost models...\n",
      "Training CatBoost models...\n",
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "#Code to create a final submission\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Calculate Winkler score for prediction intervals\n",
    "def winkler_score(y_true, y_lower, y_upper, alpha=0.2):\n",
    "    alpha = float(alpha)\n",
    "    lower_clip = np.maximum(0, (y_lower - y_true))\n",
    "    upper_clip = np.maximum(0, (y_true - y_upper))\n",
    "    interval_length = (y_upper - y_lower)\n",
    "    winkler_score = interval_length + (2/alpha) * (lower_clip + upper_clip)\n",
    "    return np.mean(winkler_score)\n",
    "\n",
    "def train_lightgbm(X_train, y_train, X_test, is_quantile=False, quantile_alpha=None):\n",
    "    params = {\n",
    "        'objective': 'regression' if not is_quantile else 'quantile',\n",
    "        'metric': 'mae' if not is_quantile else 'quantile',\n",
    "        'alpha': quantile_alpha if is_quantile else None,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'verbose': -1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "    )\n",
    "    return model, model.predict(X_test)\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test):\n",
    "    rf_model = RandomForestRegressor(n_estimators=300, min_samples_split=2, min_samples_leaf=2,\n",
    "                                    max_features='log2', max_depth=20, random_state=39, n_jobs=-1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    return rf_model, rf_model.predict(X_test)\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_test, is_quantile=False, quantile_alpha=None):\n",
    "    fixed_params = {\n",
    "        'objective': 'reg:quantileerror' if is_quantile else 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    if is_quantile:\n",
    "        fixed_params['quantile_alpha'] = quantile_alpha\n",
    "\n",
    "    params = {**fixed_params, **{'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 200,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 1.0}}\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    #Removed eval_set to avoid errors, as final test set has no target\n",
    "    model.fit(X_train, y_train, verbose=False)\n",
    "    return model, model.predict(X_test)\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, cat_features, is_quantile=False, quantile_alpha=None):\n",
    "    params = {\n",
    "        'loss_function': f'Quantile:alpha={quantile_alpha}' if is_quantile else 'MAE',\n",
    "        'eval_metric': 'Quantile' if is_quantile else 'MAE',\n",
    "        'iterations': 500,\n",
    "        'learning_rate': 0.1,\n",
    "        'random_seed': 42,\n",
    "        'verbose': 0,\n",
    "        'cat_features': cat_features\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(**params)\n",
    "    train_pool = Pool(data=X_train, label=y_train, cat_features=cat_features)\n",
    "    model.fit(train_pool) #Removed eval_set\n",
    "    return model, model.predict(X_test)\n",
    "\n",
    "\n",
    "def build_base_models(X_train_scaled, X_train_unscaled, y_train, X_test_scaled, X_test_unscaled, alpha, cat_features):\n",
    "    # --- LightGBM Model ---\n",
    "    print(\"Training LightGBM models...\")\n",
    "    lightgbm_model, y_pred_lgb = train_lightgbm(X_train_scaled, y_train, X_test_scaled)\n",
    "    _, y_lower_lgb = train_lightgbm(X_train_scaled, y_train, X_test_scaled, is_quantile=True, quantile_alpha=alpha/2)\n",
    "    _, y_upper_lgb = train_lightgbm(X_train_scaled, y_train, X_test_scaled, is_quantile=True, quantile_alpha=1-alpha/2)\n",
    "\n",
    "    # --- Random Forest Model ---\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model, y_pred_rf = train_random_forest(X_train_scaled, y_train, X_test_scaled)\n",
    "\n",
    "    # --- XGBoost Model ---\n",
    "    print(\"Training XGBoost models...\")\n",
    "    xgboost_model, y_pred_xgb = train_xgboost(X_train_scaled, y_train, X_test_scaled)\n",
    "    _, y_lower_xgb = train_xgboost(X_train_scaled, y_train, X_test_scaled, is_quantile=True, quantile_alpha=alpha/2)\n",
    "    _, y_upper_xgb = train_xgboost(X_train_scaled, y_train, X_test_scaled, is_quantile=True, quantile_alpha=1-alpha/2)\n",
    "\n",
    "    # --- CatBoost Model ---\n",
    "    print(\"Training CatBoost models...\")\n",
    "    catboost_model, y_pred_cat = train_catboost(X_train_unscaled, y_train, X_test_unscaled, cat_features)\n",
    "    _, y_lower_cat = train_catboost(X_train_unscaled, y_train, X_test_unscaled, cat_features, is_quantile=True, quantile_alpha=alpha/2)\n",
    "    _, y_upper_cat = train_catboost(X_train_unscaled, y_train, X_test_unscaled, cat_features, is_quantile=True, quantile_alpha=1-alpha/2)\n",
    "\n",
    "    return (lightgbm_model, rf_model, xgboost_model, catboost_model), (y_pred_lgb, y_lower_lgb, y_upper_lgb, y_pred_rf, y_pred_xgb, y_lower_xgb, y_upper_xgb, y_pred_cat, y_lower_cat, y_upper_cat)\n",
    "\n",
    "\n",
    "def build_final_models(train_df, test_df, final_test_df, target='price', alpha=0.2):\n",
    "    print(\"Preparing data for final modeling...\")\n",
    "    # Data Preparation for training\n",
    "    X_train = train_df.drop(['id', target], axis=1, errors='ignore')\n",
    "    y_train = train_df[target]\n",
    "    X_test = test_df.drop(['id', target], axis=1, errors='ignore')\n",
    "    y_test = test_df[target] #This one will be appended\n",
    "\n",
    "    # Data Preparation for final test set, dropping 'postcode' and target (if exists)\n",
    "    X_final_test = final_test_df.drop(['id', 'postcode', target], axis=1, errors='ignore')\n",
    "    id_final_test = final_test_df['id']\n",
    "\n",
    "     # Combine train and original test for the 'new' final training set\n",
    "    combined_train_df = pd.concat([X_train, X_test], axis=0)\n",
    "    combined_y_train = pd.concat([y_train, y_test], axis = 0)  # combine y as well\n",
    "\n",
    "\n",
    "    cat_columns = combined_train_df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Label encoding for categorical features\n",
    "    print(\"Encoding categorical features...\")\n",
    "    label_encoders = {}\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on combined train data, transform both\n",
    "        combined_train_df[col] = le.fit_transform(combined_train_df[col].astype(str))\n",
    "\n",
    "        #Check if the column exists in X_final_test, if it does, then transform it.\n",
    "        if col in X_final_test.columns:\n",
    "            # Use .transform, not .fit_transform on X_final_test\n",
    "             X_final_test[col] = le.transform(X_final_test[col].astype(str))\n",
    "\n",
    "        label_encoders[col] = le\n",
    "    # Keep unscaled copies for CatBoost\n",
    "    X_train_unscaled = combined_train_df.copy()\n",
    "    X_test_unscaled = X_final_test.copy() #keep also test data unscaled for catboost\n",
    "    cat_features_indices = [combined_train_df.columns.get_loc(col) for col in cat_columns]\n",
    "\n",
    "\n",
    "    # Fill missing values with the median of combined_train_df\n",
    "    combined_train_df = combined_train_df.fillna(combined_train_df.median())\n",
    "    X_final_test = X_final_test.fillna(combined_train_df.median())\n",
    "\n",
    "\n",
    "    # Scale features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on combined train data, transform both\n",
    "    X_train_scaled = scaler.fit_transform(combined_train_df)\n",
    "    X_test_scaled = scaler.transform(X_final_test)\n",
    "\n",
    "\n",
    "    # Convert back to DataFrame with cleaned column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=combined_train_df.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_final_test.columns)\n",
    "\n",
    "    # Clean column names for compatibility\n",
    "    def clean_column_names(df):\n",
    "        df.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in df.columns]\n",
    "        return df\n",
    "\n",
    "    X_train_scaled = clean_column_names(X_train_scaled)\n",
    "    X_test_scaled = clean_column_names(X_test_scaled)\n",
    "    X_train_unscaled = clean_column_names(X_train_unscaled)\n",
    "    X_test_unscaled = clean_column_names(X_test_unscaled)\n",
    "\n",
    "\n",
    "    #Equal weights for all models\n",
    "    best_weights = {'lgb': 0.25, 'rf': 0.25, 'xgb': 0.25, 'cat': 0.25}\n",
    "    best_k = 1e-05 #k-value\n",
    "\n",
    "    # Build final models\n",
    "    base_models, base_preds = build_base_models(X_train_scaled, X_train_unscaled, combined_y_train,\n",
    "                                               X_test_scaled, X_test_unscaled,\n",
    "                                               alpha, cat_features_indices)  #X_test already contains only the data of the final test set\n",
    "    lightgbm_model, rf_model, xgboost_model, catboost_model = base_models\n",
    "    y_pred_lgb, y_lower_lgb, y_upper_lgb, y_pred_rf, y_pred_xgb, y_lower_xgb, y_upper_xgb, y_pred_cat, y_lower_cat, y_upper_cat = base_preds\n",
    "\n",
    "\n",
    "    y_pred_combined = (\n",
    "        best_weights['lgb'] * y_pred_lgb +\n",
    "        best_weights['rf'] * y_pred_rf +\n",
    "        best_weights['xgb'] * y_pred_xgb +\n",
    "        best_weights['cat'] * y_pred_cat\n",
    "    )\n",
    "\n",
    "    # Weighted combination of *differences*\n",
    "    diff_rf = np.abs(y_pred_combined - y_pred_rf)\n",
    "    diff_lgb = np.abs(y_pred_combined - y_pred_lgb)\n",
    "    diff_cat = np.abs(y_pred_combined - y_pred_cat)\n",
    "    diff_xgb = np.abs(y_pred_combined - y_pred_xgb)\n",
    "\n",
    "    combined_diff = (\n",
    "        best_weights['rf'] * diff_rf +\n",
    "        best_weights['lgb'] * diff_lgb +\n",
    "        best_weights['cat'] * diff_cat +\n",
    "        best_weights['xgb'] * diff_xgb\n",
    "    )\n",
    "\n",
    "    # Combine upper and lower quantiles\n",
    "    y_lower_combined = (\n",
    "        best_weights['lgb'] * y_lower_lgb +\n",
    "        best_weights['rf'] * y_lower_lgb +\n",
    "        best_weights['xgb'] * y_lower_xgb +\n",
    "        best_weights['cat'] * y_lower_cat\n",
    "    )\n",
    "\n",
    "    y_upper_combined = (\n",
    "        best_weights['lgb'] * y_upper_lgb +\n",
    "        best_weights['rf'] * y_upper_lgb +\n",
    "        best_weights['xgb'] * y_upper_xgb +\n",
    "        best_weights['cat'] * y_upper_cat\n",
    "    )\n",
    "\n",
    "    avg_prediction = np.mean(np.abs(y_pred_combined))\n",
    "    scaled_diff = combined_diff / (avg_prediction + 1e-8)\n",
    "    scaling_factor = np.exp(-best_k * scaled_diff)\n",
    "\n",
    "    # Create final prediction intervals\n",
    "    initial_interval_width = y_upper_combined - y_lower_combined\n",
    "    adjusted_interval_width = initial_interval_width * scaling_factor\n",
    "\n",
    "    y_pred_final = y_pred_combined\n",
    "    y_lower_final = np.maximum(0, y_pred_final - adjusted_interval_width / 2)  #Ensure bounds aren't below 0\n",
    "    y_upper_final = np.maximum(0, y_pred_final + adjusted_interval_width / 2)\n",
    "\n",
    "    # Ensure point prediction is within bounds.  This is important!\n",
    "    y_lower_final = np.minimum(y_lower_final, y_pred_final)\n",
    "    y_upper_final = np.maximum(y_upper_final, y_pred_final)\n",
    "\n",
    "\n",
    "    # Create the final submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': id_final_test,\n",
    "        'LOWER': y_lower_final.round().astype(int),  # Round and convert to integers\n",
    "        'UPPER': y_upper_final.round().astype(int),\n",
    "        'PRED': y_pred_final.round().astype(int)\n",
    "    })\n",
    "\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting price prediction model...\")\n",
    "    train_df = pd.read_csv(\"../adv_analytics_business/data/train_data_0322.csv\", index_col=0)\n",
    "    test_df = pd.read_csv(\"../adv_analytics_business/data/test_data_0322.csv\", index_col=0)\n",
    "    final_test_df = pd.read_csv(\"../adv_analytics_business/data/orig_test_data_0322.csv\", index_col=0)\n",
    "    alpha_value = 0.2\n",
    "\n",
    "\n",
    "    # Build and evaluate models, get the submission DataFrame\n",
    "    submission_df = build_final_models(train_df, test_df, final_test_df, alpha=alpha_value)\n",
    "\n",
    "    # Save the submission file\n",
    "    submission_df.to_csv(\"submission2.csv\", index=False, header=False, quoting=0)\n",
    "\n",
    "\n",
    "    print(\"Submission file created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_analytics_bsns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

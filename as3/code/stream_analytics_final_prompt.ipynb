{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562710da",
   "metadata": {},
   "source": [
    "## Assignment 03\n",
    "### Streaming Analytics on Text Data\n",
    "\n",
    "Here we set up a pyspark cluster on local machine, then send the recieved JSON to Gemini 2.5 LLM API with a preset prompt and record the outputs. upto 1000 fre LLM API calls per day, we will limit our loop to 50 requests at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a8614",
   "metadata": {},
   "source": [
    "### Setting Up Streaming with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1299d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 14:06:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "import threading\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ArxivStreamingLLM\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1f61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"aid\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"summary\", StringType()) \\\n",
    "    .add(\"main_category\", StringType()) \\\n",
    "    .add(\"categories\", StringType()) \\\n",
    "    .add(\"published\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b55ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:06:28 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Raw stream\n",
    "raw_stream_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"seppe.net\") \\\n",
    "    .option(\"port\", 7778) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697dd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each line of raw text into structured JSON\n",
    "# Parse JSON\n",
    "json_stream_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcadb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "received_rows = []\n",
    "\n",
    "def handle_batch(df, batch_id):\n",
    "    global received_rows\n",
    "    pandas_df = df.toPandas()\n",
    "    if not pandas_df.empty:\n",
    "        for _, row in pandas_df.iterrows():\n",
    "            received_rows.append(row.to_dict())\n",
    "            print(\"\\n New article received:\\n\", row.to_dict())\n",
    "            if len(received_rows) >= 1:\n",
    "                # Stop the stream in another thread to avoid Spark deadlock\n",
    "                threading.Thread(target=query.stop).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fcfcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:06:28 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-31dd06c8-42dc-4683-a02e-d46c647f9d79. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:06:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.16575v1', 'title': 'Data Center Model for Transient Stability Analysis of Power Systems', 'summary': 'The rising demand of computing power leads to the installation of a large\\nnumber of Data Centers (DCs). Their Fault-Ride-Through (FRT) behavior and their\\nunique power characteristics, especially for DCs catered to Artificial\\nIntelligence (AI) workloads, pose a threat to the stability of power systems.\\nTo ensure its stability, it is required accurate models of the loads involved.\\nHere we propose a dynamic load model that properly captures the behaviour of\\nDCs. Its three most defining features are the use of an Uninterrupted Power\\nSupply (UPS) which sits between the server load and the grid, the cooling load\\nrepresented by an induction motor, and a pulsing load that represents the\\ntransients caused by contemporary DCs with significant AI workloads. The\\nfeatures of the proposed model and its impact on the dynamic performance of\\ntransmission systems are illustrated through a model of the all-island Irish\\ntransmission system and real-world data of the DCs currently connected to this\\nsystem.', 'main_category': 'eess.SY', 'categories': 'eess.SY,cs.SY', 'published': '2025-05-22T12:07:19Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.16576v1', 'title': 'EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic\\n  Claims by Emulating Human Actions', 'summary': 'Determining the veracity of atomic claims is an imperative component of many\\nrecently proposed fact-checking systems. Many approaches tackle this problem by\\nfirst retrieving evidence by querying a search engine and then performing\\nclassification by providing the evidence set and atomic claim to a large\\nlanguage model, but this process deviates from what a human would do in order\\nto perform the task. Recent work attempted to address this issue by proposing\\niterative evidence retrieval, allowing for evidence to be collected several\\ntimes and only when necessary. Continuing along this line of research, we\\npropose a novel claim verification system, called EMULATE, which is designed to\\nbetter emulate human actions through the use of a multi-agent framework where\\neach agent performs a small part of the larger task, such as ranking search\\nresults according to predefined criteria or evaluating webpage content.\\nExtensive experiments on several benchmarks show clear improvements over prior\\nwork, demonstrating the efficacy of our new multi-agent framework.', 'main_category': 'cs.CL', 'categories': 'cs.CL', 'published': '2025-05-22T12:08:08Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.16577v1', 'title': 'Large Language Model-Empowered Interactive Load Forecasting', 'summary': 'The growing complexity of power systems has made accurate load forecasting\\nmore important than ever. An increasing number of advanced load forecasting\\nmethods have been developed. However, the static design of current methods\\noffers no mechanism for human-model interaction. As the primary users of\\nforecasting models, system operators often find it difficult to understand and\\napply these advanced models, which typically requires expertise in artificial\\nintelligence (AI). This also prevents them from incorporating their experience\\nand real-world contextual understanding into the forecasting process. Recent\\nbreakthroughs in large language models (LLMs) offer a new opportunity to\\naddress this issue. By leveraging their natural language understanding and\\nreasoning capabilities, we propose an LLM-based multi-agent collaboration\\nframework to bridge the gap between human operators and forecasting models. A\\nset of specialized agents is designed to perform different tasks in the\\nforecasting workflow and collaborate via a dedicated communication mechanism.\\nThis framework embeds interactive mechanisms throughout the load forecasting\\npipeline, reducing the technical threshold for non-expert users and enabling\\nthe integration of human experience. Our experiments demonstrate that the\\ninteractive load forecasting accuracy can be significantly improved when users\\nprovide proper insight in key stages. Our cost analysis shows that the\\nframework remains affordable, making it practical for real-world deployment.', 'main_category': 'cs.LG', 'categories': 'cs.LG', 'published': '2025-05-22T12:11:10Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.16578v1', 'title': 'Standard Running, \"Physical Running\" and Cosmological Constant', 'summary': 'Recently it is asserted that the standard beta function does not describe the\\ncorrect running of the coupling constant in some theories. We show that the\\nproblem arises from the assumption $\\\\mu=p$ ($\\\\mu$ is a renormalization point)\\nand that a suitable choice of $\\\\mu$ gives the correct running. It is also\\nclaimed that neither the cosmological constant nor Newton constant run. We\\nargue that running can be discussed when we consider the curved spacetime.', 'main_category': 'hep-th', 'categories': 'hep-th,gr-qc', 'published': '2025-05-22T12:13:48Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.16579v1', 'title': 'Bridging the Dynamic Perception Gap: Training-Free Draft\\n  Chain-of-Thought for Dynamic Multimodal Spatial Reasoning', 'summary': 'While chains-of-thought (CoT) have advanced complex reasoning in multimodal\\nlarge language models (MLLMs), existing methods remain confined to text or\\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\\ninput images, significantly outperforms conventional approaches, offering new\\ninsights into spatial reasoning in evolving environments. To generalize this\\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\\nframework that seamlessly integrates textual CoT with corresponding visual\\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\\nenhances performance across diverse tasks, establishing a robust baseline for\\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\\nat https://github.com/Cratileo/D2R.', 'main_category': 'cs.AI', 'categories': 'cs.AI,cs.CV', 'published': '2025-05-22T12:14:23Z'}\n"
     ]
    }
   ],
   "source": [
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(handle_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545a7dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aid': 'http://arxiv.org/abs/2505.16575v1', 'title': 'Data Center Model for Transient Stability Analysis of Power Systems', 'summary': 'The rising demand of computing power leads to the installation of a large\\nnumber of Data Centers (DCs). Their Fault-Ride-Through (FRT) behavior and their\\nunique power characteristics, especially for DCs catered to Artificial\\nIntelligence (AI) workloads, pose a threat to the stability of power systems.\\nTo ensure its stability, it is required accurate models of the loads involved.\\nHere we propose a dynamic load model that properly captures the behaviour of\\nDCs. Its three most defining features are the use of an Uninterrupted Power\\nSupply (UPS) which sits between the server load and the grid, the cooling load\\nrepresented by an induction motor, and a pulsing load that represents the\\ntransients caused by contemporary DCs with significant AI workloads. The\\nfeatures of the proposed model and its impact on the dynamic performance of\\ntransmission systems are illustrated through a model of the all-island Irish\\ntransmission system and real-world data of the DCs currently connected to this\\nsystem.', 'main_category': 'eess.SY', 'categories': 'eess.SY,cs.SY', 'published': '2025-05-22T12:07:19Z'}\n"
     ]
    }
   ],
   "source": [
    "print(received_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cc5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import getpass\n",
    "\n",
    "# Go here to get your free api key: https://aistudio.google.com/app/apikey\n",
    "\n",
    "api_key = getpass.getpass(\"Enter your Gemini API key (Go here to generate your own: https://aistudio.google.com/app/apikey): \")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac8a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def predict_categories(article_json: dict) -> dict:\n",
    "    # Read base prompt from file\n",
    "    with open(\"../assets/llm_prompt9\", \"r\", encoding=\"utf-8\") as f:\n",
    "        base_prompt = f.read()\n",
    "    \n",
    "    # Create full prompt\n",
    "    json_str = json.dumps(article_json, separators=(\",\", \":\"))\n",
    "    full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "    # Send to Gemini\n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    # Try to parse result into dict if possible\n",
    "    try:\n",
    "        # Strip backticks and optional json marker\n",
    "        raw = response.text.strip()\n",
    "        if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\")  # Remove all backticks\n",
    "            raw = raw.replace(\"json\", \"\", 1).strip()  # Remove language marker\n",
    "        prediction = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse JSON response. Raw output:\")\n",
    "        print(response.text)\n",
    "        return {\"error\": \"unparsable\", \"raw_output\": response.text}\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8be740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted categories: {'main_category': 'eess.SY', 'categories': 'eess.SY,cs.SY'}\n"
     ]
    }
   ],
   "source": [
    "first_article = received_rows[0]\n",
    "result = predict_categories(first_article)\n",
    "\n",
    "print(\"Predicted categories:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eed5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 0/5 [00:00<?, ?it/s]25/05/26 14:06:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d0e35c14-fa73-4451-9847-4b66ecec2f60. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:06:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Processing articles:  80%|████████  | 4/5 [00:21<00:05,  5.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global result collector\n",
    "results = []\n",
    "\n",
    "# Target number of requests\n",
    "MAX_REQUESTS = 5\n",
    "\n",
    "# Setup tqdm in a side thread\n",
    "def track_progress():\n",
    "    with tqdm(total=MAX_REQUESTS, desc=\"Processing articles\", position=0) as pbar:\n",
    "        last = 0\n",
    "        while len(results) < MAX_REQUESTS:\n",
    "            current = len(results)\n",
    "            pbar.update(current - last)\n",
    "            last = current\n",
    "            time.sleep(0.5)  # Update every half second\n",
    "\n",
    "# Start the progress bar tracking in a separate thread\n",
    "progress_thread = threading.Thread(target=track_progress)\n",
    "progress_thread.start()\n",
    "\n",
    "# Spark streaming callback\n",
    "def process_batch(df, batch_id):\n",
    "    global results\n",
    "    pandas_df = df.toPandas()\n",
    "    for _, row in pandas_df.iterrows():\n",
    "        if len(results) >= MAX_REQUESTS:\n",
    "            threading.Thread(target=query.stop).start()\n",
    "            return\n",
    "\n",
    "        article = row.to_dict()\n",
    "        prediction = predict_categories(article)\n",
    "\n",
    "        results.append({\n",
    "            \"Aid\": article.get(\"aid\"),\n",
    "            \"Title\": article.get(\"title\"),\n",
    "            \"Main Category\": prediction.get(\"main_category\", \"N/A\"),\n",
    "            \"Categories\": prediction.get(\"categories\", \"N/A\"),\n",
    "            \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "            \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "        })\n",
    "\n",
    "        time.sleep(4.1)  # Stay within 15 RPM\n",
    "\n",
    "# Start the Spark stream\n",
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "progress_thread.join()  # Wait for tqdm to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdeef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aid</th>\n",
       "      <th>Title</th>\n",
       "      <th>Main Category</th>\n",
       "      <th>Categories</th>\n",
       "      <th>True Main Category</th>\n",
       "      <th>True Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2505.16575v1</td>\n",
       "      <td>Data Center Model for Transient Stability Anal...</td>\n",
       "      <td>eess.SY</td>\n",
       "      <td>eess.SY,cs.SY</td>\n",
       "      <td>eess.SY</td>\n",
       "      <td>eess.SY,cs.SY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2505.16576v1</td>\n",
       "      <td>EMULATE: A Multi-Agent Framework for Determini...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2505.16577v1</td>\n",
       "      <td>Large Language Model-Empowered Interactive Loa...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,cs.AI</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2505.16578v1</td>\n",
       "      <td>Standard Running, \"Physical Running\" and Cosmo...</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,gr-qc</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,gr-qc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2505.16579v1</td>\n",
       "      <td>Bridging the Dynamic Perception Gap: Training-...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI,cs.CV</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI,cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Aid  \\\n",
       "0  http://arxiv.org/abs/2505.16575v1   \n",
       "1  http://arxiv.org/abs/2505.16576v1   \n",
       "2  http://arxiv.org/abs/2505.16577v1   \n",
       "3  http://arxiv.org/abs/2505.16578v1   \n",
       "4  http://arxiv.org/abs/2505.16579v1   \n",
       "\n",
       "                                               Title Main Category  \\\n",
       "0  Data Center Model for Transient Stability Anal...       eess.SY   \n",
       "1  EMULATE: A Multi-Agent Framework for Determini...         cs.CL   \n",
       "2  Large Language Model-Empowered Interactive Loa...         cs.LG   \n",
       "3  Standard Running, \"Physical Running\" and Cosmo...        hep-th   \n",
       "4  Bridging the Dynamic Perception Gap: Training-...         cs.AI   \n",
       "\n",
       "      Categories True Main Category True Categories  \n",
       "0  eess.SY,cs.SY            eess.SY   eess.SY,cs.SY  \n",
       "1          cs.CL              cs.CL           cs.CL  \n",
       "2    cs.LG,cs.AI              cs.LG           cs.LG  \n",
       "3   hep-th,gr-qc             hep-th    hep-th,gr-qc  \n",
       "4    cs.AI,cs.CV              cs.AI     cs.AI,cs.CV  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39728d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"../outputs/predicted_categories_final_prompt.json\", orient=\"records\", indent=2)\n",
    "df.to_csv(\"../outputs/predicted_categories_final_prompt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778daf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub_dict\n",
    "with open(\"../assets/sub_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sub_dict = eval(f.read())\n",
    "\n",
    "# Load main_dict\n",
    "with open(\"../assets/main_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    main_dict = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274dcecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tags added to sub_list: ['eess.SY']\n"
     ]
    }
   ],
   "source": [
    "# List of all subcategories\n",
    "sub_list = list(sub_dict.keys()) + list(main_dict.keys())\n",
    "\n",
    "# Extract all predicted and true subcategories\n",
    "all_used_tags = set()\n",
    "\n",
    "for col in [\"Categories\", \"True Categories\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.update(map(str.strip, str(x).split(\",\"))))\n",
    "\n",
    "for col in [\"Main Category\", \"True Main Category\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.add(str(x).strip()))\n",
    "\n",
    "# Add missing tags to sub_list\n",
    "missing = [tag for tag in all_used_tags if tag not in sub_list]\n",
    "print(\"Missing tags added to sub_list:\", missing)\n",
    "sub_list += missing\n",
    "\n",
    "# Map subcategory to main category\n",
    "sub_to_main_map = {sub: sub.split(\".\")[0] if \".\" in sub else sub for sub in sub_list}\n",
    "\n",
    "# Function to map sub to main\n",
    "def sub_to_main(sub):\n",
    "    return sub_to_main_map.get(sub, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5acae413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error function\n",
    "# Error term for same main category:\n",
    "A = 0.5\n",
    "\n",
    "# Error term for different main category:\n",
    "B = 1\n",
    "\n",
    "# Multiplier for the highlighted prediction:\n",
    "C = 3\n",
    "\n",
    "# Multiplier for the other predictions:\n",
    "D = 1\n",
    "\n",
    "def sub_vs_sub_error(sub1, sub2, a = A, b = B, printing = False):\n",
    "    sub1 = str(sub1)\n",
    "    assert isinstance(sub1, str), f\"Error: Subcategory {sub1} was given, which is not a string.\"\n",
    "    assert sub1 in sub_list, f\"Error: Subcategory {sub1} was given, which is not an acceptable subcategory.\"\n",
    "    \n",
    "    sub2 = str(sub2)\n",
    "    assert isinstance(sub2, str), f\"Error: Subcategory {sub2} was given, which is not a string.\"\n",
    "    assert sub2 in sub_list, f\"Error: Subcategory {sub2} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    if sub1 == sub2:\n",
    "        return 0\n",
    "\n",
    "    main1 = sub_to_main(sub1)\n",
    "    main2 = sub_to_main(sub2)\n",
    "\n",
    "    if main1 == main2:\n",
    "        return a\n",
    "\n",
    "    return b\n",
    "\n",
    "def list_of_subs_vs_list_of_subs_error(list1, list2, a = A, b = B, printing = False):\n",
    "    if not isinstance(list1, list):\n",
    "        list1 = list1.split(\",\")\n",
    "    assert isinstance(list1, list), f\"Error: List of categories {list1} was given, which is not a list.\"\n",
    "\n",
    "    if not isinstance(list2, list):\n",
    "        list2 = list2.split(\",\")\n",
    "    assert isinstance(list2, list), f\"Error: List of categories {list2} was given, which is not a list.\"\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        list1[i] = str(list1[i])\n",
    "        assert isinstance(list1[i], str), f\"Error: Subcategory {list1[i]} was given, which is not a string.\"\n",
    "        assert list1[i] in sub_list, f\"Error: Subcategory {list1[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        list2[i] = str(list2[i])\n",
    "        assert isinstance(list2[i], str), f\"Error: Subcategory {list2[i]} was given, which is not a string.\"\n",
    "        assert list2[i] in sub_list, f\"Error: Subcategory {list2[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    # We delete the exact same subcategories present in both lists.\n",
    "    intersection1 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection1]\n",
    "    list2 = [x for x in list2 if x not in intersection1]\n",
    "\n",
    "    mains_present_in_both = [sub_to_main(sub) for sub in intersection1]\n",
    "    list1 = [sub_to_main(sub) for sub in list1]\n",
    "    list2 = [sub_to_main(sub) for sub in list2]\n",
    "    mains_present_in_both += (set(list1) & set(list2))\n",
    "    if printing:\n",
    "        print(mains_present_in_both, list1, list2)\n",
    "\n",
    "    half_bad1 = [sub1 for sub1 in list1 if sub1 in mains_present_in_both]\n",
    "    half_bad2 = [sub2 for sub2 in list2 if sub2 in mains_present_in_both]\n",
    "    if printing:\n",
    "        print(half_bad1, half_bad2)\n",
    "\n",
    "    list1 = [sub1 for sub1 in list1 if sub1 not in half_bad1]\n",
    "    list2 = [sub2 for sub2 in list2 if sub2 not in half_bad2]\n",
    "\n",
    "    intersection2 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection2]\n",
    "    list2 = [x for x in list2 if x not in intersection2]\n",
    "    if printing:\n",
    "        print(len(intersection1), len(intersection2), len(list1), len(list2), len(half_bad1), len(half_bad2))\n",
    "\n",
    "    return a * (len(intersection2) + len(half_bad1) + len(half_bad2)) + b * (len(list1) + len(list2))\n",
    "\n",
    "def pred_vs_pred_error(pred1, pred2, a = A, b = B, c = C, d = D, printing = False):\n",
    "    assert isinstance(pred1, dict), f\"Error: Prediction {pred1} was given, which is not a dictionary.\"\n",
    "    assert isinstance(pred2, dict), f\"Error: Prediction {pred2} was given, which is not a dictionary.\"\n",
    "    highlighted_pred1 = pred1[\"main_category\"]\n",
    "    highlighted_pred2 = pred2[\"main_category\"]\n",
    "    others_pred1 = pred1[\"categories\"]\n",
    "    others_pred2 = pred2[\"categories\"]\n",
    "\n",
    "    return c * sub_vs_sub_error(highlighted_pred1, highlighted_pred2, printing = printing) + d * list_of_subs_vs_list_of_subs_error(others_pred1, others_pred2, printing = printing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5fd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"Main Category\"],\n",
    "        \"categories\": row[\"Categories\"]\n",
    "    }\n",
    "\n",
    "def make_true_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"True Main Category\"],\n",
    "        \"categories\": row[\"True Categories\"]\n",
    "    }\n",
    "\n",
    "df[\"Error\"] = df.apply(lambda row: pred_vs_pred_error(make_pred_dict(row), make_true_dict(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef658b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ../assets/llm_prompt9 with gemini-2.0-flash\n",
      "Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:06:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6e3d8f4b-8dd9-43ba-84f8-4934ad1987b7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:06:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter1.csv\n",
      "Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:08:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-89511389-5551-49fa-82c1-34e96ba1bb87. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:08:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter2.csv\n",
      "Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:10:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0cc78e3d-7402-46db-a8ec-c49b51d51d91. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:10:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter3.csv\n",
      "Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:12:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-81bdde7e-fab8-4086-aec1-ec8852bb2fa8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:12:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter4.csv\n",
      "Summary saved as prompt_model_mean_errors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>mean_error_gemini-2_0-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../assets/llm_prompt9</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              llm_prompt  mean_error_gemini-2_0-flash\n",
       "0  ../assets/llm_prompt9                        0.206"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"gemini-2.0-flash\"]\n",
    "prompts = [f\"../assets/llm_prompt{i}\" for i in [9]]\n",
    "NUM_ITERATIONS = 4\n",
    "MAX_REQUESTS = 20\n",
    "error_summary = []\n",
    "\n",
    "for prompt_file in prompts:\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning {prompt_file} with {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        iteration_errors = []\n",
    "\n",
    "        for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "            print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "            received_rows = []\n",
    "\n",
    "            # === Fetch 30 JSONs from stream ===\n",
    "            def handle_batch(df, batch_id):\n",
    "                global received_rows\n",
    "                pdf = df.toPandas()\n",
    "                for _, r in pdf.iterrows():\n",
    "                    received_rows.append(r.to_dict())\n",
    "                    if len(received_rows) >= MAX_REQUESTS:\n",
    "                        threading.Thread(target=query.stop).start()\n",
    "\n",
    "            query = json_stream_df.writeStream.foreachBatch(handle_batch).start()\n",
    "            query.awaitTermination()\n",
    "\n",
    "            if len(received_rows) < MAX_REQUESTS:\n",
    "                print(\"Skipping this iteration (not enough samples)\")\n",
    "                continue\n",
    "\n",
    "            # === Prediction and Scoring ===\n",
    "            eval_results = []\n",
    "            with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                base_prompt = f.read()\n",
    "\n",
    "            for article in received_rows[:MAX_REQUESTS]:\n",
    "                json_str = json.dumps(article, separators=(\",\", \":\"))\n",
    "                full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "                try:\n",
    "                    response = model.generate_content(full_prompt)\n",
    "                    raw = response.text.strip()\n",
    "                    if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "                        raw = raw.strip(\"`\").replace(\"json\", \"\", 1).strip()\n",
    "                    pred = json.loads(raw)\n",
    "                except Exception:\n",
    "                    pred = {\"main_category\": \"unknown\", \"categories\": \"unknown\"}\n",
    "\n",
    "                eval_results.append({\n",
    "                    \"Aid\": article.get(\"aid\"),\n",
    "                    \"Title\": article.get(\"title\"),\n",
    "                    \"Main Category\": pred.get(\"main_category\", \"N/A\"),\n",
    "                    \"Categories\": pred.get(\"categories\", \"N/A\"),\n",
    "                    \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "                    \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "                })\n",
    "                time.sleep(4.1)\n",
    "\n",
    "            df_temp = pd.DataFrame(eval_results)\n",
    "\n",
    "            # Error handling\n",
    "            errors = []\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                try:\n",
    "                    pred = {\n",
    "                        \"main_category\": row[\"Main Category\"],\n",
    "                        \"categories\": row[\"Categories\"]\n",
    "                    }\n",
    "                    true = {\n",
    "                        \"main_category\": row[\"True Main Category\"],\n",
    "                        \"categories\": row[\"True Categories\"]\n",
    "                    }\n",
    "                    err = pred_vs_pred_error(pred, true)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping row {idx} due to category error: {e}\")\n",
    "                    err = None\n",
    "                errors.append(err)\n",
    "\n",
    "            df_temp[\"Error\"] = errors\n",
    "            df_temp_clean = df_temp.dropna(subset=[\"Error\"])\n",
    "\n",
    "            # Save per-iteration detailed results\n",
    "            fname = f\"../outputs/{prompt_file}_{model_name.replace('.', '_')}_iter{iteration}.csv\"\n",
    "            df_temp.to_csv(fname, index=False)\n",
    "            print(f\"Saved {fname}\")\n",
    "\n",
    "            if not df_temp_clean.empty:\n",
    "                iteration_errors.append(df_temp_clean[\"Error\"].mean())\n",
    "\n",
    "        # Store final mean error for this prompt-model combo\n",
    "        mean_error = round(sum(iteration_errors) / len(iteration_errors), 3) if iteration_errors else None\n",
    "        error_summary.append({\n",
    "            \"llm_prompt\": prompt_file,\n",
    "            f\"mean_error_{model_name.replace('.', '_')}\": mean_error\n",
    "        })\n",
    "\n",
    "# === Final summary ===\n",
    "df_summary = pd.DataFrame(error_summary)\n",
    "df_summary.to_csv(\"../outputs/prompt_model_mean_error_final_prompt.csv\", index=False)\n",
    "print(\"Summary saved as prompt_model_mean_errors.csv\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74d834",
   "metadata": {},
   "source": [
    "### Testing the final prompt with updated error metric coefficients for comparison to Bart LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9c9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ../assets/llm_prompt9 with gemini-2.0-flash\n",
      "Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:14:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e41e322b-e458-4b2f-a67b-f19000289d02. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:14:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter1.csv\n",
      "Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:15:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f422c197-58dd-4025-9d09-dabfa7b97675. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:15:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter2.csv\n",
      "Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:17:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0748bf05-bb79-40b2-b60a-ce787bd4ce10. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:17:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter3.csv\n",
      "Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 14:19:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1f02ab31-5b57-411e-a596-b3b99e32c32b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/26 14:19:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt9_gemini-2_0-flash_iter4.csv\n",
      "Summary saved as prompt_model_mean_errors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>mean_error_gemini-2_0-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../assets/llm_prompt9</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              llm_prompt  mean_error_gemini-2_0-flash\n",
       "0  ../assets/llm_prompt9                        0.169"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Updated Error Coefficients\n",
    "# Error term for same main category:\n",
    "A = 0.5\n",
    "\n",
    "# Error term for different main category:\n",
    "B = 1\n",
    "\n",
    "# Multiplier for the highlighted prediction:\n",
    "C = 1\n",
    "\n",
    "# Multiplier for the other predictions:\n",
    "D = 0\n",
    "\n",
    "models = [\"gemini-2.0-flash\"]\n",
    "prompts = [f\"../assets/llm_prompt{i}\" for i in [9]]\n",
    "NUM_ITERATIONS = 4\n",
    "MAX_REQUESTS = 20\n",
    "error_summary = []\n",
    "\n",
    "for prompt_file in prompts:\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning {prompt_file} with {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        iteration_errors = []\n",
    "\n",
    "        for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "            print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "            received_rows = []\n",
    "\n",
    "            # === Fetch 30 JSONs from stream ===\n",
    "            def handle_batch(df, batch_id):\n",
    "                global received_rows\n",
    "                pdf = df.toPandas()\n",
    "                for _, r in pdf.iterrows():\n",
    "                    received_rows.append(r.to_dict())\n",
    "                    if len(received_rows) >= MAX_REQUESTS:\n",
    "                        threading.Thread(target=query.stop).start()\n",
    "\n",
    "            query = json_stream_df.writeStream.foreachBatch(handle_batch).start()\n",
    "            query.awaitTermination()\n",
    "\n",
    "            if len(received_rows) < MAX_REQUESTS:\n",
    "                print(\"Skipping this iteration (not enough samples)\")\n",
    "                continue\n",
    "\n",
    "            # === Prediction and Scoring ===\n",
    "            eval_results = []\n",
    "            with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                base_prompt = f.read()\n",
    "\n",
    "            for article in received_rows[:MAX_REQUESTS]:\n",
    "                json_str = json.dumps(article, separators=(\",\", \":\"))\n",
    "                full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "                try:\n",
    "                    response = model.generate_content(full_prompt)\n",
    "                    raw = response.text.strip()\n",
    "                    if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "                        raw = raw.strip(\"`\").replace(\"json\", \"\", 1).strip()\n",
    "                    pred = json.loads(raw)\n",
    "                except Exception:\n",
    "                    pred = {\"main_category\": \"unknown\", \"categories\": \"unknown\"}\n",
    "\n",
    "                eval_results.append({\n",
    "                    \"Aid\": article.get(\"aid\"),\n",
    "                    \"Title\": article.get(\"title\"),\n",
    "                    \"Main Category\": pred.get(\"main_category\", \"N/A\"),\n",
    "                    \"Categories\": pred.get(\"categories\", \"N/A\"),\n",
    "                    \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "                    \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "                })\n",
    "                time.sleep(4.1)\n",
    "\n",
    "            df_temp = pd.DataFrame(eval_results)\n",
    "\n",
    "            # Error handling\n",
    "            errors = []\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                try:\n",
    "                    pred = {\n",
    "                        \"main_category\": row[\"Main Category\"],\n",
    "                        \"categories\": row[\"Categories\"]\n",
    "                    }\n",
    "                    true = {\n",
    "                        \"main_category\": row[\"True Main Category\"],\n",
    "                        \"categories\": row[\"True Categories\"]\n",
    "                    }\n",
    "                    err = pred_vs_pred_error(pred, true)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping row {idx} due to category error: {e}\")\n",
    "                    err = None\n",
    "                errors.append(err)\n",
    "\n",
    "            df_temp[\"Error\"] = errors\n",
    "            df_temp_clean = df_temp.dropna(subset=[\"Error\"])\n",
    "\n",
    "            # Save per-iteration detailed results\n",
    "            fname = f\"../outputs/{prompt_file}_{model_name.replace('.', '_')}_iter{iteration}.csv\"\n",
    "            df_temp.to_csv(fname, index=False)\n",
    "            print(f\"Saved {fname}\")\n",
    "\n",
    "            if not df_temp_clean.empty:\n",
    "                iteration_errors.append(df_temp_clean[\"Error\"].mean())\n",
    "\n",
    "        # Store final mean error for this prompt-model combo\n",
    "        mean_error = round(sum(iteration_errors) / len(iteration_errors), 3) if iteration_errors else None\n",
    "        error_summary.append({\n",
    "            \"llm_prompt\": prompt_file,\n",
    "            f\"mean_error_{model_name.replace('.', '_')}\": mean_error\n",
    "        })\n",
    "\n",
    "# === Final summary ===\n",
    "df_summary = pd.DataFrame(error_summary)\n",
    "df_summary.to_csv(\"../outputs/prompt_model_mean_error_final_prompt.csv\", index=False)\n",
    "print(\"Summary saved as prompt_model_mean_errors.csv\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f9ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_analytics_bsns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

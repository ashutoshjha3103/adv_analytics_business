{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562710da",
   "metadata": {},
   "source": [
    "## Assignment 03\n",
    "### Streaming Analytics on Text Data\n",
    "\n",
    "Here we set up a pyspark cluster on local machine, then send the recieved JSON to Gemini 2.5 LLM API with a preset prompt and record the outputs. upto 1000 fre LLM API calls per day, we will limit our loop to 50 requests at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a8614",
   "metadata": {},
   "source": [
    "### Setting Up Streaming with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1299d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/29 15:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "import threading\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ArxivStreamingLLM\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1f61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"aid\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"summary\", StringType()) \\\n",
    "    .add(\"main_category\", StringType()) \\\n",
    "    .add(\"categories\", StringType()) \\\n",
    "    .add(\"published\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b55ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:20:35 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Raw stream\n",
    "raw_stream_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"seppe.net\") \\\n",
    "    .option(\"port\", 7778) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697dd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each line of raw text into structured JSON\n",
    "# Parse JSON\n",
    "json_stream_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcadb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "received_rows = []\n",
    "\n",
    "def handle_batch(df, batch_id):\n",
    "    global received_rows\n",
    "    pandas_df = df.toPandas()\n",
    "    if not pandas_df.empty:\n",
    "        for _, row in pandas_df.iterrows():\n",
    "            received_rows.append(row.to_dict())\n",
    "            print(\"\\n New article received:\\n\", row.to_dict())\n",
    "            if len(received_rows) >= 1:\n",
    "                # Stop the stream in another thread to avoid Spark deadlock\n",
    "                threading.Thread(target=query.stop).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fcfcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:20:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1ee99627-b927-4caf-a7da-f58fce8142e8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:20:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.19411v1', 'title': 'Split-as-a-Pro: behavioral control via operator splitting and\\n  alternating projections', 'summary': 'The paper introduces Split-as-a-Pro, a control framework that integrates\\nbehavioral systems theory, operator splitting methods, and alternating\\nprojection algorithms. The framework reduces dynamic optimization problems -\\narising in both control and estimation - to efficient projection computations.\\nSplit-as-a-Pro builds on a non-parametric formulation that exploits system\\nstructure to separate dynamic constraints imposed by individual subsystems from\\nexternal ones, such as interconnection constraints and input/output\\nconstraints. This enables the use of arbitrary system representations, as long\\nas the associated projection is efficiently computable, thereby enhancing\\nscalability and compatibility with gray-box modeling. We demonstrate the\\neffectiveness of Split-as-a-Pro by developing a distributed algorithm for\\nsolving finite-horizon linear quadratic control problems and illustrate its use\\nin predictive control. Our numerical case studies show that algorithms obtained\\nusing Split-as-a-Pro significantly outperform their centralized counterparts in\\nruntime and scalability across various standard graph topologies, while\\nseamlessly leveraging both model-based and data-driven system representations.', 'main_category': 'math.OC', 'categories': 'math.OC,cs.SY,eess.SY', 'published': '2025-05-26T02:02:43Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.19412v1', 'title': '2d Cardy-Rabinovici model with the modified Villain lattice: Exact\\n  dualities and symmetries', 'summary': 'The Cardy-Rabinovici model is a toy model of the lattice $U(1)$ gauge\\ntheories to study various oblique confinement states associated with the\\nnonzero $\\\\theta$ angles. We reformulate the $2$d version of this model using\\nthe modified Villain lattice formalism, and we establish the exact $\\\\theta$\\nperiodicity for the Witten effect and the strong-weak duality at the finite\\nlattice spacings. We then study the phase structure of this model based on the\\nduality, symmetry and anomaly, and the perturbative renormalization group.', 'main_category': 'hep-th', 'categories': 'hep-th,hep-lat', 'published': '2025-05-26T02:04:31Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.19413v1', 'title': 'Limit distributions for $\\\\text{SO}(n,1)$ action on $k$-lattices in\\n  $\\\\mathbb{R}^{n+1}$', 'summary': 'We study the asymptotic distribution of norm ball averages along orbits of a\\nlattice $\\\\Gamma \\\\subset \\\\text{SO}(n,1)$ acting on the moduli space of pairs of\\northogonal discrete subgroups of $\\\\mathbb{R}^{n+1}$ up to homothety. Our main\\nresult shows that, except for special $2$-lattices in $\\\\mathbb{R}^3$ lying in\\nhyperplanes tangent to the light cone, these measures converge to an explicit\\nsemi-invariant probability measure supported on the space of homothety classes\\nof pairs of orthogonal lattices tangent to the light cone.\\n  Our main motivation is a conjecture of Sargent and Shapira, which is resolved\\nas a special case of our general result.', 'main_category': 'math.DS', 'categories': 'math.DS', 'published': '2025-05-26T02:06:29Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.19414v1', 'title': 'Toward Physics-Informed Machine Learning for Data Center Operations: A\\n  Tropical Case Study', 'summary': 'Data centers are the backbone of computing capacity. Operating data centers\\nin the tropical regions faces unique challenges due to consistently high\\nambient temperature and elevated relative humidity throughout the year. These\\nconditions result in increased cooling costs to maintain the reliability of the\\ncomputing systems. While existing machine learning-based approaches have\\ndemonstrated potential to elevate operations to a more proactive and\\nintelligent level, their deployment remains dubious due to concerns about model\\nextrapolation capabilities and associated system safety issues. To address\\nthese concerns, this article proposes incorporating the physical\\ncharacteristics of data centers into traditional data-driven machine learning\\nsolutions. We begin by introducing the data center system, including the\\nrelevant multiphysics processes and the data-physics availability. Next, we\\noutline the associated modeling and optimization problems and propose an\\nintegrated, physics-informed machine learning system to address them. Using the\\nproposed system, we present relevant applications across varying levels of\\noperational intelligence. A case study on an industry-grade tropical data\\ncenter is provided to demonstrate the effectiveness of our approach. Finally,\\nwe discuss key challenges and highlight potential future directions.', 'main_category': 'cs.AI', 'categories': 'cs.AI,cs.LG', 'published': '2025-05-26T02:06:45Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.19415v1', 'title': 'MMIG-Bench: Towards Comprehensive and Explainable Evaluation of\\n  Multi-Modal Image Generation Models', 'summary': 'Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\\nGemini 2.5 Pro excel at following complex instructions, editing images and\\nmaintaining concept consistency. However, they are still evaluated by disjoint\\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\\nand customized image generation benchmarks that overlook compositional\\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\\n4,850 richly annotated text prompts with 1,750 multi-view reference images\\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\\nmetrics for visual artifacts and identity preservation of objects; (2) novel\\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\\nfine-grained prompt-image alignment and shows strong correlation with human\\njudgments; and (3) high-level metrics for aesthetics and human preference.\\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\\nratings, yielding in-depth insights into architecture and data design. We will\\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\\nand accelerate future innovations in multi-modal image generation.', 'main_category': 'cs.CV', 'categories': 'cs.CV', 'published': '2025-05-26T02:07:24Z'}\n"
     ]
    }
   ],
   "source": [
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(handle_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545a7dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aid': 'http://arxiv.org/abs/2505.19411v1', 'title': 'Split-as-a-Pro: behavioral control via operator splitting and\\n  alternating projections', 'summary': 'The paper introduces Split-as-a-Pro, a control framework that integrates\\nbehavioral systems theory, operator splitting methods, and alternating\\nprojection algorithms. The framework reduces dynamic optimization problems -\\narising in both control and estimation - to efficient projection computations.\\nSplit-as-a-Pro builds on a non-parametric formulation that exploits system\\nstructure to separate dynamic constraints imposed by individual subsystems from\\nexternal ones, such as interconnection constraints and input/output\\nconstraints. This enables the use of arbitrary system representations, as long\\nas the associated projection is efficiently computable, thereby enhancing\\nscalability and compatibility with gray-box modeling. We demonstrate the\\neffectiveness of Split-as-a-Pro by developing a distributed algorithm for\\nsolving finite-horizon linear quadratic control problems and illustrate its use\\nin predictive control. Our numerical case studies show that algorithms obtained\\nusing Split-as-a-Pro significantly outperform their centralized counterparts in\\nruntime and scalability across various standard graph topologies, while\\nseamlessly leveraging both model-based and data-driven system representations.', 'main_category': 'math.OC', 'categories': 'math.OC,cs.SY,eess.SY', 'published': '2025-05-26T02:02:43Z'}\n"
     ]
    }
   ],
   "source": [
    "print(received_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cc5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import getpass\n",
    "\n",
    "# Go here to get your free api key: https://aistudio.google.com/app/apikey\n",
    "\n",
    "api_key = getpass.getpass(\"Enter your Gemini API key (Go here to generate your own: https://aistudio.google.com/app/apikey): \")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac8a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def predict_categories(article_json: dict) -> dict:\n",
    "    # Read base prompt from file\n",
    "    with open(\"../assets/llm_prompt4\", \"r\", encoding=\"utf-8\") as f:\n",
    "        base_prompt = f.read()\n",
    "    \n",
    "    # Create full prompt\n",
    "    json_str = json.dumps(article_json, separators=(\",\", \":\"))\n",
    "    full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "    # Send to Gemini\n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    # Try to parse result into dict if possible\n",
    "    try:\n",
    "        # Strip backticks and optional json marker\n",
    "        raw = response.text.strip()\n",
    "        if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\")  # Remove all backticks\n",
    "            raw = raw.replace(\"json\", \"\", 1).strip()  # Remove language marker\n",
    "        prediction = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse JSON response. Raw output:\")\n",
    "        print(response.text)\n",
    "        return {\"error\": \"unparsable\", \"raw_output\": response.text}\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8be740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted categories: {'main_category': 'math.OC', 'categories': 'math.OC,cs.SY,eess.SY'}\n"
     ]
    }
   ],
   "source": [
    "first_article = received_rows[0]\n",
    "result = predict_categories(first_article)\n",
    "\n",
    "print(\"Predicted categories:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eed5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:21:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-18af8050-58f0-48fa-a22b-9abd31d4e2ed. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:21:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Processing articles:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:  80%|████████  | 4/5 [00:21<00:05,  5.38s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global result collector\n",
    "results = []\n",
    "\n",
    "# Target number of requests\n",
    "MAX_REQUESTS = 5\n",
    "\n",
    "# Setup tqdm in a side thread\n",
    "def track_progress():\n",
    "    with tqdm(total=MAX_REQUESTS, desc=\"Processing articles\", position=0) as pbar:\n",
    "        last = 0\n",
    "        while len(results) < MAX_REQUESTS:\n",
    "            current = len(results)\n",
    "            pbar.update(current - last)\n",
    "            last = current\n",
    "            time.sleep(0.5)  # Update every half second\n",
    "\n",
    "# Start the progress bar tracking in a separate thread\n",
    "progress_thread = threading.Thread(target=track_progress)\n",
    "progress_thread.start()\n",
    "\n",
    "# Spark streaming callback\n",
    "def process_batch(df, batch_id):\n",
    "    global results\n",
    "    pandas_df = df.toPandas()\n",
    "    for _, row in pandas_df.iterrows():\n",
    "        if len(results) >= MAX_REQUESTS:\n",
    "            threading.Thread(target=query.stop).start()\n",
    "            return\n",
    "\n",
    "        article = row.to_dict()\n",
    "        prediction = predict_categories(article)\n",
    "\n",
    "        results.append({\n",
    "            \"Aid\": article.get(\"aid\"),\n",
    "            \"Title\": article.get(\"title\"),\n",
    "            \"Main Category\": prediction.get(\"main_category\", \"N/A\"),\n",
    "            \"Categories\": prediction.get(\"categories\", \"N/A\"),\n",
    "            \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "            \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "        })\n",
    "\n",
    "        time.sleep(4.1)  # Stay within 15 RPM\n",
    "\n",
    "# Start the Spark stream\n",
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "progress_thread.join()  # Wait for tqdm to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdeef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aid</th>\n",
       "      <th>Title</th>\n",
       "      <th>Main Category</th>\n",
       "      <th>Categories</th>\n",
       "      <th>True Main Category</th>\n",
       "      <th>True Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2505.19411v1</td>\n",
       "      <td>Split-as-a-Pro: behavioral control via operato...</td>\n",
       "      <td>math.OC</td>\n",
       "      <td>math.OC,cs.SY,eess.SY</td>\n",
       "      <td>math.OC</td>\n",
       "      <td>math.OC,cs.SY,eess.SY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2505.19412v1</td>\n",
       "      <td>2d Cardy-Rabinovici model with the modified Vi...</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,hep-lat</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,hep-lat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2505.19413v1</td>\n",
       "      <td>Limit distributions for $\\text{SO}(n,1)$ actio...</td>\n",
       "      <td>math.DS</td>\n",
       "      <td>math.DS</td>\n",
       "      <td>math.DS</td>\n",
       "      <td>math.DS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2505.19429v1</td>\n",
       "      <td>Rhapsody: A Dataset for Highlight Detection in...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2505.19414v1</td>\n",
       "      <td>Toward Physics-Informed Machine Learning for D...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI,cs.LG</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI,cs.LG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Aid  \\\n",
       "0  http://arxiv.org/abs/2505.19411v1   \n",
       "1  http://arxiv.org/abs/2505.19412v1   \n",
       "2  http://arxiv.org/abs/2505.19413v1   \n",
       "3  http://arxiv.org/abs/2505.19429v1   \n",
       "4  http://arxiv.org/abs/2505.19414v1   \n",
       "\n",
       "                                               Title Main Category  \\\n",
       "0  Split-as-a-Pro: behavioral control via operato...       math.OC   \n",
       "1  2d Cardy-Rabinovici model with the modified Vi...        hep-th   \n",
       "2  Limit distributions for $\\text{SO}(n,1)$ actio...       math.DS   \n",
       "3  Rhapsody: A Dataset for Highlight Detection in...         cs.CL   \n",
       "4  Toward Physics-Informed Machine Learning for D...         cs.AI   \n",
       "\n",
       "              Categories True Main Category        True Categories  \n",
       "0  math.OC,cs.SY,eess.SY            math.OC  math.OC,cs.SY,eess.SY  \n",
       "1         hep-th,hep-lat             hep-th         hep-th,hep-lat  \n",
       "2                math.DS            math.DS                math.DS  \n",
       "3                  cs.CL              cs.CL                  cs.CL  \n",
       "4            cs.AI,cs.LG              cs.AI            cs.AI,cs.LG  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a39728d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"../outputs/predicted_categories_final_prompt.json\", orient=\"records\", indent=2)\n",
    "df.to_csv(\"../outputs/predicted_categories_final_prompt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "778daf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub_dict\n",
    "with open(\"../assets/sub_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sub_dict = eval(f.read())\n",
    "\n",
    "# Load main_dict\n",
    "with open(\"../assets/main_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    main_dict = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274dcecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tags added to sub_list: ['eess.SY']\n"
     ]
    }
   ],
   "source": [
    "# List of all subcategories\n",
    "sub_list = list(sub_dict.keys()) + list(main_dict.keys())\n",
    "\n",
    "# Extract all predicted and true subcategories\n",
    "all_used_tags = set()\n",
    "\n",
    "for col in [\"Categories\", \"True Categories\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.update(map(str.strip, str(x).split(\",\"))))\n",
    "\n",
    "for col in [\"Main Category\", \"True Main Category\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.add(str(x).strip()))\n",
    "\n",
    "# Add missing tags to sub_list\n",
    "missing = [tag for tag in all_used_tags if tag not in sub_list]\n",
    "print(\"Missing tags added to sub_list:\", missing)\n",
    "sub_list += missing\n",
    "\n",
    "# Map subcategory to main category\n",
    "sub_to_main_map = {sub: sub.split(\".\")[0] if \".\" in sub else sub for sub in sub_list}\n",
    "\n",
    "# Function to map sub to main\n",
    "def sub_to_main(sub):\n",
    "    return sub_to_main_map.get(sub, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5acae413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error function\n",
    "# Error term for same main category:\n",
    "A = 0.5\n",
    "\n",
    "# Error term for different main category:\n",
    "B = 1\n",
    "\n",
    "# Multiplier for the highlighted prediction:\n",
    "C = 3\n",
    "\n",
    "# Multiplier for the other predictions:\n",
    "D = 1\n",
    "\n",
    "def sub_vs_sub_error(sub1, sub2, a = A, b = B, printing = False):\n",
    "    sub1 = str(sub1)\n",
    "    assert isinstance(sub1, str), f\"Error: Subcategory {sub1} was given, which is not a string.\"\n",
    "    assert sub1 in sub_list, f\"Error: Subcategory {sub1} was given, which is not an acceptable subcategory.\"\n",
    "    \n",
    "    sub2 = str(sub2)\n",
    "    assert isinstance(sub2, str), f\"Error: Subcategory {sub2} was given, which is not a string.\"\n",
    "    assert sub2 in sub_list, f\"Error: Subcategory {sub2} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    if sub1 == sub2:\n",
    "        return 0\n",
    "\n",
    "    main1 = sub_to_main(sub1)\n",
    "    main2 = sub_to_main(sub2)\n",
    "\n",
    "    if main1 == main2:\n",
    "        return a\n",
    "\n",
    "    return b\n",
    "\n",
    "def list_of_subs_vs_list_of_subs_error(list1, list2, a = A, b = B, printing = False):\n",
    "    if not isinstance(list1, list):\n",
    "        list1 = list1.split(\",\")\n",
    "    assert isinstance(list1, list), f\"Error: List of categories {list1} was given, which is not a list.\"\n",
    "\n",
    "    if not isinstance(list2, list):\n",
    "        list2 = list2.split(\",\")\n",
    "    assert isinstance(list2, list), f\"Error: List of categories {list2} was given, which is not a list.\"\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        list1[i] = str(list1[i])\n",
    "        assert isinstance(list1[i], str), f\"Error: Subcategory {list1[i]} was given, which is not a string.\"\n",
    "        assert list1[i] in sub_list, f\"Error: Subcategory {list1[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        list2[i] = str(list2[i])\n",
    "        assert isinstance(list2[i], str), f\"Error: Subcategory {list2[i]} was given, which is not a string.\"\n",
    "        assert list2[i] in sub_list, f\"Error: Subcategory {list2[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    # We delete the exact same subcategories present in both lists.\n",
    "    intersection1 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection1]\n",
    "    list2 = [x for x in list2 if x not in intersection1]\n",
    "\n",
    "    mains_present_in_both = [sub_to_main(sub) for sub in intersection1]\n",
    "    list1 = [sub_to_main(sub) for sub in list1]\n",
    "    list2 = [sub_to_main(sub) for sub in list2]\n",
    "    mains_present_in_both += (set(list1) & set(list2))\n",
    "    if printing:\n",
    "        print(mains_present_in_both, list1, list2)\n",
    "\n",
    "    half_bad1 = [sub1 for sub1 in list1 if sub1 in mains_present_in_both]\n",
    "    half_bad2 = [sub2 for sub2 in list2 if sub2 in mains_present_in_both]\n",
    "    if printing:\n",
    "        print(half_bad1, half_bad2)\n",
    "\n",
    "    list1 = [sub1 for sub1 in list1 if sub1 not in half_bad1]\n",
    "    list2 = [sub2 for sub2 in list2 if sub2 not in half_bad2]\n",
    "\n",
    "    intersection2 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection2]\n",
    "    list2 = [x for x in list2 if x not in intersection2]\n",
    "    if printing:\n",
    "        print(len(intersection1), len(intersection2), len(list1), len(list2), len(half_bad1), len(half_bad2))\n",
    "\n",
    "    return a * (len(intersection2) + len(half_bad1) + len(half_bad2)) + b * (len(list1) + len(list2))\n",
    "\n",
    "def pred_vs_pred_error(pred1, pred2, a = A, b = B, c = C, d = D, printing = False):\n",
    "    assert isinstance(pred1, dict), f\"Error: Prediction {pred1} was given, which is not a dictionary.\"\n",
    "    assert isinstance(pred2, dict), f\"Error: Prediction {pred2} was given, which is not a dictionary.\"\n",
    "    highlighted_pred1 = pred1[\"main_category\"]\n",
    "    highlighted_pred2 = pred2[\"main_category\"]\n",
    "    others_pred1 = pred1[\"categories\"]\n",
    "    others_pred2 = pred2[\"categories\"]\n",
    "\n",
    "    return c * sub_vs_sub_error(highlighted_pred1, highlighted_pred2, printing = printing) + d * list_of_subs_vs_list_of_subs_error(others_pred1, others_pred2, printing = printing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5fd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"Main Category\"],\n",
    "        \"categories\": row[\"Categories\"]\n",
    "    }\n",
    "\n",
    "def make_true_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"True Main Category\"],\n",
    "        \"categories\": row[\"True Categories\"]\n",
    "    }\n",
    "\n",
    "df[\"Error\"] = df.apply(lambda row: pred_vs_pred_error(make_pred_dict(row), make_true_dict(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef658b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ../assets/llm_prompt4 with gemini-2.0-flash\n",
      "Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:21:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-25332647-6a6d-46ea-be9e-c1f842c69395. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:21:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter1.csv\n",
      "Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:23:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9ae22129-1081-46cd-9baf-c5a320a77a98. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:23:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter2.csv\n",
      "Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:25:16 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6482bd5a-1c88-4c1e-9ddd-61b395faf055. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:25:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter3.csv\n",
      "Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:27:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-23febe70-1cd9-4add-aff6-67ca71456a50. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:27:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter4.csv\n",
      "Summary saved as prompt_model_mean_errors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>mean_error_gemini-2_0-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../assets/llm_prompt4</td>\n",
       "      <td>0.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              llm_prompt  mean_error_gemini-2_0-flash\n",
       "0  ../assets/llm_prompt4                        0.181"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"gemini-2.0-flash\"]\n",
    "prompts = [f\"../assets/llm_prompt{i}\" for i in [4]]\n",
    "NUM_ITERATIONS = 4\n",
    "MAX_REQUESTS = 20\n",
    "error_summary = []\n",
    "\n",
    "for prompt_file in prompts:\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning {prompt_file} with {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        iteration_errors = []\n",
    "\n",
    "        for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "            print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "            received_rows = []\n",
    "\n",
    "            # === Fetch 30 JSONs from stream ===\n",
    "            def handle_batch(df, batch_id):\n",
    "                global received_rows\n",
    "                pdf = df.toPandas()\n",
    "                for _, r in pdf.iterrows():\n",
    "                    received_rows.append(r.to_dict())\n",
    "                    if len(received_rows) >= MAX_REQUESTS:\n",
    "                        threading.Thread(target=query.stop).start()\n",
    "\n",
    "            query = json_stream_df.writeStream.foreachBatch(handle_batch).start()\n",
    "            query.awaitTermination()\n",
    "\n",
    "            if len(received_rows) < MAX_REQUESTS:\n",
    "                print(\"Skipping this iteration (not enough samples)\")\n",
    "                continue\n",
    "\n",
    "            # === Prediction and Scoring ===\n",
    "            eval_results = []\n",
    "            with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                base_prompt = f.read()\n",
    "\n",
    "            for article in received_rows[:MAX_REQUESTS]:\n",
    "                json_str = json.dumps(article, separators=(\",\", \":\"))\n",
    "                full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "                try:\n",
    "                    response = model.generate_content(full_prompt)\n",
    "                    raw = response.text.strip()\n",
    "                    if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "                        raw = raw.strip(\"`\").replace(\"json\", \"\", 1).strip()\n",
    "                    pred = json.loads(raw)\n",
    "                except Exception:\n",
    "                    pred = {\"main_category\": \"unknown\", \"categories\": \"unknown\"}\n",
    "\n",
    "                eval_results.append({\n",
    "                    \"Aid\": article.get(\"aid\"),\n",
    "                    \"Title\": article.get(\"title\"),\n",
    "                    \"Main Category\": pred.get(\"main_category\", \"N/A\"),\n",
    "                    \"Categories\": pred.get(\"categories\", \"N/A\"),\n",
    "                    \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "                    \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "                })\n",
    "                time.sleep(4.1)\n",
    "\n",
    "            df_temp = pd.DataFrame(eval_results)\n",
    "\n",
    "            # Error handling\n",
    "            errors = []\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                try:\n",
    "                    pred = {\n",
    "                        \"main_category\": row[\"Main Category\"],\n",
    "                        \"categories\": row[\"Categories\"]\n",
    "                    }\n",
    "                    true = {\n",
    "                        \"main_category\": row[\"True Main Category\"],\n",
    "                        \"categories\": row[\"True Categories\"]\n",
    "                    }\n",
    "                    err = pred_vs_pred_error(pred, true)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping row {idx} due to category error: {e}\")\n",
    "                    err = None\n",
    "                errors.append(err)\n",
    "\n",
    "            df_temp[\"Error\"] = errors\n",
    "            df_temp_clean = df_temp.dropna(subset=[\"Error\"])\n",
    "\n",
    "            # Save per-iteration detailed results\n",
    "            fname = f\"../outputs/{prompt_file}_{model_name.replace('.', '_')}_iter{iteration}.csv\"\n",
    "            df_temp.to_csv(fname, index=False)\n",
    "            print(f\"Saved {fname}\")\n",
    "\n",
    "            if not df_temp_clean.empty:\n",
    "                iteration_errors.append(df_temp_clean[\"Error\"].mean())\n",
    "\n",
    "        # Store final mean error for this prompt-model combo\n",
    "        mean_error = round(sum(iteration_errors) / len(iteration_errors), 3) if iteration_errors else None\n",
    "        error_summary.append({\n",
    "            \"llm_prompt\": prompt_file,\n",
    "            f\"mean_error_{model_name.replace('.', '_')}\": mean_error\n",
    "        })\n",
    "\n",
    "# === Final summary ===\n",
    "df_summary = pd.DataFrame(error_summary)\n",
    "df_summary.to_csv(\"../outputs/prompt_model_mean_error_final_prompt.csv\", index=False)\n",
    "print(\"Summary saved as prompt_model_mean_errors.csv\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74d834",
   "metadata": {},
   "source": [
    "### Testing the final prompt with updated error metric coefficients for comparison to Bart LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9c9b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running ../assets/llm_prompt4 with gemini-2.0-flash\n",
      "Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:28:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-aca6db23-537c-457f-b41d-c24bdd6aa3bb. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:28:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter1.csv\n",
      "Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:30:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e17deb4e-b3e3-4353-bb43-6cd299a5d7e1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:30:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter2.csv\n",
      "Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:32:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7fd891d6-c36b-45d1-a5ec-de29755c9510. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:32:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter3.csv\n",
      "Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/29 15:34:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-eacf8fc6-60de-41df-baae-6d1dfb1f721a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/29 15:34:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../outputs/../assets/llm_prompt4_gemini-2_0-flash_iter4.csv\n",
      "Summary saved as prompt_model_mean_errors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>mean_error_gemini-2_0-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../assets/llm_prompt4</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              llm_prompt  mean_error_gemini-2_0-flash\n",
       "0  ../assets/llm_prompt4                        0.325"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Updated Error Coefficients\n",
    "# Error term for same main category:\n",
    "A = 0.5\n",
    "\n",
    "# Error term for different main category:\n",
    "B = 1\n",
    "\n",
    "# Multiplier for the highlighted prediction:\n",
    "C = 1\n",
    "\n",
    "# Multiplier for the other predictions:\n",
    "D = 0\n",
    "\n",
    "models = [\"gemini-2.0-flash\"]\n",
    "prompts = [f\"../assets/llm_prompt{i}\" for i in [4]]\n",
    "NUM_ITERATIONS = 4\n",
    "MAX_REQUESTS = 20\n",
    "error_summary = []\n",
    "\n",
    "for prompt_file in prompts:\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning {prompt_file} with {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        iteration_errors = []\n",
    "\n",
    "        for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "            print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "            received_rows = []\n",
    "\n",
    "            # === Fetch 30 JSONs from stream ===\n",
    "            def handle_batch(df, batch_id):\n",
    "                global received_rows\n",
    "                pdf = df.toPandas()\n",
    "                for _, r in pdf.iterrows():\n",
    "                    received_rows.append(r.to_dict())\n",
    "                    if len(received_rows) >= MAX_REQUESTS:\n",
    "                        threading.Thread(target=query.stop).start()\n",
    "\n",
    "            query = json_stream_df.writeStream.foreachBatch(handle_batch).start()\n",
    "            query.awaitTermination()\n",
    "\n",
    "            if len(received_rows) < MAX_REQUESTS:\n",
    "                print(\"Skipping this iteration (not enough samples)\")\n",
    "                continue\n",
    "\n",
    "            # === Prediction and Scoring ===\n",
    "            eval_results = []\n",
    "            with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                base_prompt = f.read()\n",
    "\n",
    "            for article in received_rows[:MAX_REQUESTS]:\n",
    "                json_str = json.dumps(article, separators=(\",\", \":\"))\n",
    "                full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "                try:\n",
    "                    response = model.generate_content(full_prompt)\n",
    "                    raw = response.text.strip()\n",
    "                    if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "                        raw = raw.strip(\"`\").replace(\"json\", \"\", 1).strip()\n",
    "                    pred = json.loads(raw)\n",
    "                except Exception:\n",
    "                    pred = {\"main_category\": \"unknown\", \"categories\": \"unknown\"}\n",
    "\n",
    "                eval_results.append({\n",
    "                    \"Aid\": article.get(\"aid\"),\n",
    "                    \"Title\": article.get(\"title\"),\n",
    "                    \"Main Category\": pred.get(\"main_category\", \"N/A\"),\n",
    "                    \"Categories\": pred.get(\"categories\", \"N/A\"),\n",
    "                    \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "                    \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "                })\n",
    "                time.sleep(4.1)\n",
    "\n",
    "            df_temp = pd.DataFrame(eval_results)\n",
    "\n",
    "            # Error handling\n",
    "            errors = []\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                try:\n",
    "                    pred = {\n",
    "                        \"main_category\": row[\"Main Category\"],\n",
    "                        \"categories\": row[\"Categories\"]\n",
    "                    }\n",
    "                    true = {\n",
    "                        \"main_category\": row[\"True Main Category\"],\n",
    "                        \"categories\": row[\"True Categories\"]\n",
    "                    }\n",
    "                    err = pred_vs_pred_error(pred, true)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping row {idx} due to category error: {e}\")\n",
    "                    err = None\n",
    "                errors.append(err)\n",
    "\n",
    "            df_temp[\"Error\"] = errors\n",
    "            df_temp_clean = df_temp.dropna(subset=[\"Error\"])\n",
    "\n",
    "            # Save per-iteration detailed results\n",
    "            fname = f\"../outputs/{prompt_file}_{model_name.replace('.', '_')}_iter{iteration}.csv\"\n",
    "            df_temp.to_csv(fname, index=False)\n",
    "            print(f\"Saved {fname}\")\n",
    "\n",
    "            if not df_temp_clean.empty:\n",
    "                iteration_errors.append(df_temp_clean[\"Error\"].mean())\n",
    "\n",
    "        # Store final mean error for this prompt-model combo\n",
    "        mean_error = round(sum(iteration_errors) / len(iteration_errors), 3) if iteration_errors else None\n",
    "        error_summary.append({\n",
    "            \"llm_prompt\": prompt_file,\n",
    "            f\"mean_error_{model_name.replace('.', '_')}\": mean_error\n",
    "        })\n",
    "\n",
    "# === Final summary ===\n",
    "df_summary = pd.DataFrame(error_summary)\n",
    "df_summary.to_csv(\"../outputs/prompt_model_mean_error_final_prompt.csv\", index=False)\n",
    "print(\"Summary saved as prompt_model_mean_errors.csv\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f9ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_analytics_bsns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562710da",
   "metadata": {},
   "source": [
    "## Assignment 03\n",
    "### Streaming Analytics on Text Data\n",
    "\n",
    "Here we set up a pyspark cluster on local machine, then send the recieved JSON to Gemini 2.5 LLM API with a preset prompt and record the outputs. upto 1000 fre LLM API calls per day, we will limit our loop to 50 requests at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a8614",
   "metadata": {},
   "source": [
    "### Setting Up Streaming with Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1299d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 16:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "import threading\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ArxivStreamingLLM\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1f61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"aid\", StringType()) \\\n",
    "    .add(\"title\", StringType()) \\\n",
    "    .add(\"summary\", StringType()) \\\n",
    "    .add(\"main_category\", StringType()) \\\n",
    "    .add(\"categories\", StringType()) \\\n",
    "    .add(\"published\", StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b55ca7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 16:35:58 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Raw stream\n",
    "raw_stream_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"seppe.net\") \\\n",
    "    .option(\"port\", 7778) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697dd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each line of raw text into structured JSON\n",
    "# Parse JSON\n",
    "json_stream_df = raw_stream_df \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcadb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "received_rows = []\n",
    "\n",
    "def handle_batch(df, batch_id):\n",
    "    global received_rows\n",
    "    pandas_df = df.toPandas()\n",
    "    if not pandas_df.empty:\n",
    "        for _, row in pandas_df.iterrows():\n",
    "            received_rows.append(row.to_dict())\n",
    "            print(\"\\n New article received:\\n\", row.to_dict())\n",
    "            if len(received_rows) >= 1:\n",
    "                # Stop the stream in another thread to avoid Spark deadlock\n",
    "                threading.Thread(target=query.stop).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0fcfcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/25 16:36:03 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-704182d5-ac2c-4665-a26b-ee02531e4b63. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/25 16:36:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.15582v1', 'title': 'Measurement of the Z-boson mass', 'summary': 'The first dedicated $Z$-boson mass measurement at the LHC with $Z \\\\to\\n\\\\mu^+\\\\mu^-$ decays is reported. The dataset uses proton-proton collisions at a\\ncentre-of-mass energy of $13$ TeV, recorded in 2016 by the LHCb experiment, and\\ncorresponds to an integrated luminosity of $1.7$ fb$^{-1}$. A template fit to\\nthe $\\\\mu^+\\\\mu^-$ mass distribution yields the following result for the\\n$Z$-boson mass, \\\\begin{equation*}\\n  m_{Z} = 91184.2 \\\\pm 8.5 \\\\pm 3.8 \\\\rm{MeV}, \\\\end{equation*} where the first\\nuncertainty is statistical and the second systematic. This result is consistent\\nwith previous measurements and predictions from global electroweak fits.', 'main_category': 'hep-ex', 'categories': 'hep-ex', 'published': '2025-05-21T14:38:31Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.15583v1', 'title': 'Special cycles in compact locally Hermitian symmetric spaces of type III\\n  associated with the Lie group $SO_0(2,m)$', 'summary': \"Let $G = SO_0(2,m),$ the connected component of the Lie group $SO(2,m);\\\\ K =\\nSO(2) \\\\times SO(m),$ a maximal compact subgroup of $G;$ and $\\\\theta$ be the\\nassociated Cartan involution of $G.$ Let $X = G/K,\\\\ \\\\frak{g}_0$ be the Lie\\nalgebra of $G$ and $\\\\frak{g} = \\\\frak{g}_0^\\\\mathbb{C}.$ In this article, we have\\nconsidered the special cycles associated with all possible involutions of $G$\\ncommuting with $\\\\theta.$ We have determined the special cycles which give\\nnon-zero cohomology classes in $H^*(\\\\Gamma \\\\backslash X; \\\\mathbb{C})$ for some\\n$\\\\theta$-stable torsion-free arithmetic uniform lattice $\\\\Gamma$ in $G,$ by a\\nresult of Millson and Raghunathan. For each cohomologically induced\\nrepresentation $A_\\\\frak{q}$ with trivial infinitesimal character, we have\\ndetermined the special cycles for which the non-zero cohomology class has no\\n$A_\\\\frak{q}$-component, via Matsushima's isomorphism.\", 'main_category': 'math.RT', 'categories': 'math.RT', 'published': '2025-05-21T14:39:08Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.15584v1', 'title': 'Improved power methods for computing eigenvalues of dual quaternion\\n  Hermitian matrices', 'summary': 'This paper investigates the eigenvalue computation problem of the dual\\nquaternion Hermitian matrix closely related to multi-agent group control.\\nRecently, power method was proposed by Cui and Qi in Journal of Scientific\\nComputing, 100 (2024) to solve such problem. Recognizing that the convergence\\nrate of power method is slow due to its dependence on the eigenvalue\\ndistribution, we propose two improved versions of power method based on dual\\ncomplex adjoint matrices and Aitken extrapolation, named DCAM-PM and ADCAM-PM.\\nThey achieve notable efficiency improvements and demonstrate significantly\\nfaster convergence. However, power method may be invalid for dual quaternion\\nHermitian matrices with eigenvalues having identical standard parts but\\ndistinct dual parts. To overcome this disadvantage, utilizing the\\neigen-decomposition properties of dual complex adjoint matrix, we propose a\\nnovel algorithm EDDCAM-EA which surpasses the power method in both accuracy and\\nspeed. Application to eigenvalue computations of dual quaternion Hermitian\\nmatrices in multi-agent formation control and numerical experiments highlight\\nthe remarkable accuracy and speed of our proposed algorithms.', 'main_category': 'math.NA', 'categories': 'math.NA,cs.NA', 'published': '2025-05-21T14:40:16Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.15585v1', 'title': 'MIRB: Mathematical Information Retrieval Benchmark', 'summary': 'Mathematical Information Retrieval (MIR) is the task of retrieving\\ninformation from mathematical documents and plays a key role in various\\napplications, including theorem search in mathematical libraries, answer\\nretrieval on math forums, and premise selection in automated theorem proving.\\nHowever, a unified benchmark for evaluating these diverse retrieval tasks has\\nbeen lacking. In this paper, we introduce MIRB (Mathematical Information\\nRetrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB\\nincludes four tasks: semantic statement retrieval, question-answer retrieval,\\npremise retrieval, and formula retrieval, spanning a total of 12 datasets. We\\nevaluate 13 retrieval models on this benchmark and analyze the challenges\\ninherent to MIR. We hope that MIRB provides a comprehensive framework for\\nevaluating MIR systems and helps advance the development of more effective\\nretrieval models tailored to the mathematical domain.', 'main_category': 'cs.IR', 'categories': 'cs.IR,cs.CL,cs.LG', 'published': '2025-05-21T14:40:27Z'}\n",
      "\n",
      " New article received:\n",
      " {'aid': 'http://arxiv.org/abs/2505.15586v1', 'title': 'A De Giorgi conjecture on the regularity of minimizers of Cartesian area\\n  in 1D', 'summary': 'We prove a $C^{1,1}$-regularity of minimizers of the functional $$ \\\\int_I\\n\\\\sqrt{1+|Du|^2} + \\\\int_I |u-g|ds,\\\\quad u\\\\in BV(I), $$ provided\\n$I\\\\subset\\\\mathbb{R}$ is a bounded open interval and $\\\\|g\\\\|_\\\\infty$ is\\nsufficiently small, thus partially establishing a De Giorgi conjecture in\\ndimension one and codimension one. We also extend our result to a suitable\\nanisotropic setting.', 'main_category': 'math.AP', 'categories': 'math.AP,math.CA', 'published': '2025-05-21T14:41:43Z'}\n"
     ]
    }
   ],
   "source": [
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(handle_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "545a7dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aid': 'http://arxiv.org/abs/2505.15582v1', 'title': 'Measurement of the Z-boson mass', 'summary': 'The first dedicated $Z$-boson mass measurement at the LHC with $Z \\\\to\\n\\\\mu^+\\\\mu^-$ decays is reported. The dataset uses proton-proton collisions at a\\ncentre-of-mass energy of $13$ TeV, recorded in 2016 by the LHCb experiment, and\\ncorresponds to an integrated luminosity of $1.7$ fb$^{-1}$. A template fit to\\nthe $\\\\mu^+\\\\mu^-$ mass distribution yields the following result for the\\n$Z$-boson mass, \\\\begin{equation*}\\n  m_{Z} = 91184.2 \\\\pm 8.5 \\\\pm 3.8 \\\\rm{MeV}, \\\\end{equation*} where the first\\nuncertainty is statistical and the second systematic. This result is consistent\\nwith previous measurements and predictions from global electroweak fits.', 'main_category': 'hep-ex', 'categories': 'hep-ex', 'published': '2025-05-21T14:38:31Z'}\n"
     ]
    }
   ],
   "source": [
    "print(received_rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cc5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import getpass\n",
    "\n",
    "# Go here to get your free api key: https://aistudio.google.com/app/apikey\n",
    "\n",
    "api_key = getpass.getpass(\"Enter your Gemini API key (Go here to generate your own: https://aistudio.google.com/app/apikey): \")\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "def predict_categories(article_json: dict) -> dict:\n",
    "    # Read base prompt from file\n",
    "    with open(\"../assets/llm_prompt1\", \"r\", encoding=\"utf-8\") as f:\n",
    "        base_prompt = f.read()\n",
    "    \n",
    "    # Create full prompt\n",
    "    json_str = json.dumps(article_json, separators=(\",\", \":\"))\n",
    "    full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "    # Send to Gemini\n",
    "    response = model.generate_content(full_prompt)\n",
    "    \n",
    "    # Try to parse result into dict if possible\n",
    "    try:\n",
    "        # Strip backticks and optional json marker\n",
    "        raw = response.text.strip()\n",
    "        if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "            raw = raw.strip(\"`\")  # Remove all backticks\n",
    "            raw = raw.replace(\"json\", \"\", 1).strip()  # Remove language marker\n",
    "        prediction = json.loads(raw)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse JSON response. Raw output:\")\n",
    "        print(response.text)\n",
    "        return {\"error\": \"unparsable\", \"raw_output\": response.text}\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8be740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted categories: {'main_category': 'cs.LG', 'categories': 'cs.LG,cs.AI,cs.CL,cs.CV'}\n"
     ]
    }
   ],
   "source": [
    "first_article = received_rows[0]\n",
    "result = predict_categories(first_article)\n",
    "\n",
    "print(\"Predicted categories:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eed5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 0/10 [00:00<?, ?it/s]25/05/23 15:13:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1b85194b-b84a-403c-bc57-c483aca64c03. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:13:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Processing articles:  90%|█████████ | 9/10 [00:46<00:05,  5.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Global result collector\n",
    "results = []\n",
    "\n",
    "# Target number of requests\n",
    "MAX_REQUESTS = 10\n",
    "\n",
    "# Setup tqdm in a side thread\n",
    "def track_progress():\n",
    "    with tqdm(total=MAX_REQUESTS, desc=\"Processing articles\", position=0) as pbar:\n",
    "        last = 0\n",
    "        while len(results) < MAX_REQUESTS:\n",
    "            current = len(results)\n",
    "            pbar.update(current - last)\n",
    "            last = current\n",
    "            time.sleep(0.5)  # Update every half second\n",
    "\n",
    "# Start the progress bar tracking in a separate thread\n",
    "progress_thread = threading.Thread(target=track_progress)\n",
    "progress_thread.start()\n",
    "\n",
    "# Spark streaming callback\n",
    "def process_batch(df, batch_id):\n",
    "    global results\n",
    "    pandas_df = df.toPandas()\n",
    "    for _, row in pandas_df.iterrows():\n",
    "        if len(results) >= MAX_REQUESTS:\n",
    "            threading.Thread(target=query.stop).start()\n",
    "            return\n",
    "\n",
    "        article = row.to_dict()\n",
    "        prediction = predict_categories(article)\n",
    "\n",
    "        results.append({\n",
    "            \"Aid\": article.get(\"aid\"),\n",
    "            \"Title\": article.get(\"title\"),\n",
    "            \"Main Category\": prediction.get(\"main_category\", \"N/A\"),\n",
    "            \"Categories\": prediction.get(\"categories\", \"N/A\"),\n",
    "            \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "            \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "        })\n",
    "\n",
    "        time.sleep(4.1)  # Stay within 15 RPM\n",
    "\n",
    "# Start the Spark stream\n",
    "query = json_stream_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "progress_thread.join()  # Wait for tqdm to finish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdeef1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aid</th>\n",
       "      <th>Title</th>\n",
       "      <th>Main Category</th>\n",
       "      <th>Categories</th>\n",
       "      <th>True Main Category</th>\n",
       "      <th>True Categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2505.13083v1</td>\n",
       "      <td>Seismic Isolation of Optical Tables Using Piez...</td>\n",
       "      <td>astro-ph.IM</td>\n",
       "      <td>astro-ph.IM,physics.ins-det</td>\n",
       "      <td>astro-ph.IM</td>\n",
       "      <td>astro-ph.IM,physics.ins-det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2505.13084v1</td>\n",
       "      <td>Thermodynamic parameters of fluids on conforma...</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>gr-qc,hep-th,math-ph</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>gr-qc,hep-th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2505.13085v1</td>\n",
       "      <td>Universal Semantic Disentangled Privacy-preser...</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>eess.AS,cs.LG</td>\n",
       "      <td>eess.AS</td>\n",
       "      <td>eess.AS,cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2505.13086v1</td>\n",
       "      <td>Coupled integral equations method with open bo...</td>\n",
       "      <td>physics.acc-ph</td>\n",
       "      <td>physics.acc-ph,physics.comp-ph,physics.optics</td>\n",
       "      <td>physics.acc-ph</td>\n",
       "      <td>physics.acc-ph,physics.comp-ph,physics.optics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2505.13087v1</td>\n",
       "      <td>Graph Alignment for Benchmarking Graph Neural ...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,cs.AI,cs.DM,math.CO</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://arxiv.org/abs/2505.13088v1</td>\n",
       "      <td>Cross-modal feature fusion for robust point cl...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV,cs.AI,cs.RO</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV,cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://arxiv.org/abs/2505.13089v1</td>\n",
       "      <td>Systematic Generalization in Language Models S...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL,cs.AI</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://arxiv.org/abs/2505.13090v1</td>\n",
       "      <td>The Effect of Language Diversity When Fine-Tun...</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://arxiv.org/abs/2505.13091v1</td>\n",
       "      <td>Touch2Shape: Touch-Conditioned 3D Diffusion fo...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV,cs.RO,cs.LG</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://arxiv.org/abs/2505.13092v1</td>\n",
       "      <td>Treatment Effect Estimation for Optimal Decisi...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,math.OC,cs.AI</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Aid  \\\n",
       "0  http://arxiv.org/abs/2505.13083v1   \n",
       "1  http://arxiv.org/abs/2505.13084v1   \n",
       "2  http://arxiv.org/abs/2505.13085v1   \n",
       "3  http://arxiv.org/abs/2505.13086v1   \n",
       "4  http://arxiv.org/abs/2505.13087v1   \n",
       "5  http://arxiv.org/abs/2505.13088v1   \n",
       "6  http://arxiv.org/abs/2505.13089v1   \n",
       "7  http://arxiv.org/abs/2505.13090v1   \n",
       "8  http://arxiv.org/abs/2505.13091v1   \n",
       "9  http://arxiv.org/abs/2505.13092v1   \n",
       "\n",
       "                                               Title   Main Category  \\\n",
       "0  Seismic Isolation of Optical Tables Using Piez...     astro-ph.IM   \n",
       "1  Thermodynamic parameters of fluids on conforma...           gr-qc   \n",
       "2  Universal Semantic Disentangled Privacy-preser...         eess.AS   \n",
       "3  Coupled integral equations method with open bo...  physics.acc-ph   \n",
       "4  Graph Alignment for Benchmarking Graph Neural ...           cs.LG   \n",
       "5  Cross-modal feature fusion for robust point cl...           cs.CV   \n",
       "6  Systematic Generalization in Language Models S...           cs.CL   \n",
       "7  The Effect of Language Diversity When Fine-Tun...           cs.CL   \n",
       "8  Touch2Shape: Touch-Conditioned 3D Diffusion fo...           cs.CV   \n",
       "9  Treatment Effect Estimation for Optimal Decisi...           cs.LG   \n",
       "\n",
       "                                      Categories True Main Category  \\\n",
       "0                    astro-ph.IM,physics.ins-det        astro-ph.IM   \n",
       "1                           gr-qc,hep-th,math-ph              gr-qc   \n",
       "2                                  eess.AS,cs.LG            eess.AS   \n",
       "3  physics.acc-ph,physics.comp-ph,physics.optics     physics.acc-ph   \n",
       "4                      cs.LG,cs.AI,cs.DM,math.CO              cs.LG   \n",
       "5                              cs.CV,cs.AI,cs.RO              cs.CV   \n",
       "6                                    cs.CL,cs.AI              cs.CL   \n",
       "7                                          cs.CL              cs.CL   \n",
       "8                              cs.CV,cs.RO,cs.LG              cs.CV   \n",
       "9                            cs.LG,math.OC,cs.AI              cs.LG   \n",
       "\n",
       "                                 True Categories  \n",
       "0                    astro-ph.IM,physics.ins-det  \n",
       "1                                   gr-qc,hep-th  \n",
       "2                                  eess.AS,cs.LG  \n",
       "3  physics.acc-ph,physics.comp-ph,physics.optics  \n",
       "4                                    cs.LG,cs.AI  \n",
       "5                                    cs.CV,cs.LG  \n",
       "6                                          cs.CL  \n",
       "7                                          cs.CL  \n",
       "8                                          cs.CV  \n",
       "9                                          cs.LG  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39728d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"../outputs/predicted_categories.json\", orient=\"records\", indent=2)\n",
    "df.to_csv(\"../outputs/predicted_categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778daf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sub_dict\n",
    "with open(\"../assets/sub_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sub_dict = eval(f.read())\n",
    "\n",
    "# Load main_dict\n",
    "with open(\"../assets/main_dict\", \"r\", encoding=\"utf-8\") as f:\n",
    "    main_dict = eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274dcecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing tags added to sub_list: []\n"
     ]
    }
   ],
   "source": [
    "# List of all subcategories\n",
    "sub_list = list(sub_dict.keys()) + list(main_dict.keys())\n",
    "\n",
    "# Extract all predicted and true subcategories\n",
    "all_used_tags = set()\n",
    "\n",
    "for col in [\"Categories\", \"True Categories\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.update(map(str.strip, str(x).split(\",\"))))\n",
    "\n",
    "for col in [\"Main Category\", \"True Main Category\"]:\n",
    "    df[col].dropna().apply(lambda x: all_used_tags.add(str(x).strip()))\n",
    "\n",
    "# Add missing tags to sub_list\n",
    "missing = [tag for tag in all_used_tags if tag not in sub_list]\n",
    "print(\"Missing tags added to sub_list:\", missing)\n",
    "sub_list += missing\n",
    "\n",
    "# Map subcategory to main category\n",
    "sub_to_main_map = {sub: sub.split(\".\")[0] if \".\" in sub else sub for sub in sub_list}\n",
    "\n",
    "# Function to map sub to main\n",
    "def sub_to_main(sub):\n",
    "    return sub_to_main_map.get(sub, \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5acae413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error function\n",
    "# Error term for same main category:\n",
    "A = 0.5\n",
    "\n",
    "# Error term for different main category:\n",
    "B = 1\n",
    "\n",
    "# Multiplier for the highlighted prediction:\n",
    "C = 3\n",
    "\n",
    "# Multiplier for the other predictions:\n",
    "D = 1\n",
    "\n",
    "def sub_vs_sub_error(sub1, sub2, a = A, b = B, printing = False):\n",
    "    sub1 = str(sub1)\n",
    "    assert isinstance(sub1, str), f\"Error: Subcategory {sub1} was given, which is not a string.\"\n",
    "    assert sub1 in sub_list, f\"Error: Subcategory {sub1} was given, which is not an acceptable subcategory.\"\n",
    "    \n",
    "    sub2 = str(sub2)\n",
    "    assert isinstance(sub2, str), f\"Error: Subcategory {sub2} was given, which is not a string.\"\n",
    "    assert sub2 in sub_list, f\"Error: Subcategory {sub2} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    if sub1 == sub2:\n",
    "        return 0\n",
    "\n",
    "    main1 = sub_to_main(sub1)\n",
    "    main2 = sub_to_main(sub2)\n",
    "\n",
    "    if main1 == main2:\n",
    "        return a\n",
    "\n",
    "    return b\n",
    "\n",
    "def list_of_subs_vs_list_of_subs_error(list1, list2, a = A, b = B, printing = False):\n",
    "    if not isinstance(list1, list):\n",
    "        list1 = list1.split(\",\")\n",
    "    assert isinstance(list1, list), f\"Error: List of categories {list1} was given, which is not a list.\"\n",
    "\n",
    "    if not isinstance(list2, list):\n",
    "        list2 = list2.split(\",\")\n",
    "    assert isinstance(list2, list), f\"Error: List of categories {list2} was given, which is not a list.\"\n",
    "    \n",
    "    for i in range(len(list1)):\n",
    "        list1[i] = str(list1[i])\n",
    "        assert isinstance(list1[i], str), f\"Error: Subcategory {list1[i]} was given, which is not a string.\"\n",
    "        assert list1[i] in sub_list, f\"Error: Subcategory {list1[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    for i in range(len(list2)):\n",
    "        list2[i] = str(list2[i])\n",
    "        assert isinstance(list2[i], str), f\"Error: Subcategory {list2[i]} was given, which is not a string.\"\n",
    "        assert list2[i] in sub_list, f\"Error: Subcategory {list2[i]} was given, which is not an acceptable subcategory.\"\n",
    "\n",
    "    # We delete the exact same subcategories present in both lists.\n",
    "    intersection1 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection1]\n",
    "    list2 = [x for x in list2 if x not in intersection1]\n",
    "\n",
    "    mains_present_in_both = [sub_to_main(sub) for sub in intersection1]\n",
    "    list1 = [sub_to_main(sub) for sub in list1]\n",
    "    list2 = [sub_to_main(sub) for sub in list2]\n",
    "    mains_present_in_both += (set(list1) & set(list2))\n",
    "    if printing:\n",
    "        print(mains_present_in_both, list1, list2)\n",
    "\n",
    "    half_bad1 = [sub1 for sub1 in list1 if sub1 in mains_present_in_both]\n",
    "    half_bad2 = [sub2 for sub2 in list2 if sub2 in mains_present_in_both]\n",
    "    if printing:\n",
    "        print(half_bad1, half_bad2)\n",
    "\n",
    "    list1 = [sub1 for sub1 in list1 if sub1 not in half_bad1]\n",
    "    list2 = [sub2 for sub2 in list2 if sub2 not in half_bad2]\n",
    "\n",
    "    intersection2 = set(list1) & set(list2)\n",
    "    list1 = [x for x in list1 if x not in intersection2]\n",
    "    list2 = [x for x in list2 if x not in intersection2]\n",
    "    if printing:\n",
    "        print(len(intersection1), len(intersection2), len(list1), len(list2), len(half_bad1), len(half_bad2))\n",
    "\n",
    "    return a * (len(intersection2) + len(half_bad1) + len(half_bad2)) + b * (len(list1) + len(list2))\n",
    "\n",
    "def pred_vs_pred_error(pred1, pred2, a = A, b = B, c = C, d = D, printing = False):\n",
    "    assert isinstance(pred1, dict), f\"Error: Prediction {pred1} was given, which is not a dictionary.\"\n",
    "    assert isinstance(pred2, dict), f\"Error: Prediction {pred2} was given, which is not a dictionary.\"\n",
    "    highlighted_pred1 = pred1[\"main_category\"]\n",
    "    highlighted_pred2 = pred2[\"main_category\"]\n",
    "    others_pred1 = pred1[\"categories\"]\n",
    "    others_pred2 = pred2[\"categories\"]\n",
    "\n",
    "    return c * sub_vs_sub_error(highlighted_pred1, highlighted_pred2, printing = printing) + d * list_of_subs_vs_list_of_subs_error(others_pred1, others_pred2, printing = printing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5fd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"Main Category\"],\n",
    "        \"categories\": row[\"Categories\"]\n",
    "    }\n",
    "\n",
    "def make_true_dict(row):\n",
    "    return {\n",
    "        \"main_category\": row[\"True Main Category\"],\n",
    "        \"categories\": row[\"True Categories\"]\n",
    "    }\n",
    "\n",
    "df[\"Error\"] = df.apply(lambda row: pred_vs_pred_error(make_pred_dict(row), make_true_dict(row)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19b2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global average prediction error: 0.700\n"
     ]
    }
   ],
   "source": [
    "global_error = df[\"Error\"].mean()\n",
    "print(f\"Global average prediction error: {global_error:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3802a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    10.000000\n",
      "mean      0.700000\n",
      "std       0.674949\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.750000\n",
      "75%       1.375000\n",
      "max       1.500000\n",
      "Name: Error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Error\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57c04a",
   "metadata": {},
   "source": [
    "Code to test different prompts and different llm apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce7eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running llm_prompt1 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:30:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-41c067fe-d558-40da-8b47-b6a621a19250. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:30:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 5 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 24 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt1_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:32:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-97344c18-360b-4665-a8be-3fad1f3a816a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:32:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 1 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 20 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt1_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:35:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-acd47369-58b5-4412-a8a3-712e1c240d7a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:35:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 0 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 19 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt1_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:37:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f1628626-b7d3-4596-bed4-92ecd02357f7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:37:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 13 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 20 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt1_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt2 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:39:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4937d60c-2b8f-47b6-ba9c-8fc71c790686. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:39:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 13 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 20 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt2_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:41:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6ee093a9-be18-4a34-9531-f503c7eb391b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:41:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 11 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 18 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt2_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:43:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5537c11e-5ec6-4b96-818d-5d6dbf2399be. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:43:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 10 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 17 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt2_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:46:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-24b46601-a436-4b05-b917-6a0ed6bc1d6c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:46:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 9 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 15 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt2_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt3 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:48:15 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-46d15451-4d5e-4a09-8912-60c85da82f7d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:48:15 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 7 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 14 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt3_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:50:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-70dcf48a-f8cb-4f7b-acac-70f25840950d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:50:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 3 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 10 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 22 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt3_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:52:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f89b859d-f5d3-4ab5-97bd-37e001309d7a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:52:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 0 due to category error: Error: Subcategory I.2.6 was given, which is not an acceptable subcategory.\n",
      "Skipping row 7 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 19 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt3_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:54:50 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-05f103d1-5d4a-487d-99ef-30a264f9b0c5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:54:50 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 5 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 17 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt3_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt4 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:57:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ed77b817-13a1-4a5e-a741-635f04d7c271. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:57:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 5 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 17 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt4_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 15:59:16 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c8fd6b19-4191-4f1d-9faf-67c9bb6c5c6f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 15:59:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 3 due to category error: Error: Subcategory eess.SY was given, which is not an acceptable subcategory.\n",
      "Skipping row 13 due to category error: Error: Subcategory unknown was given, which is not an acceptable subcategory.\n",
      "Skipping row 15 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt4_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:01:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fbcd5844-4c5d-4950-862a-95cd1a164031. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:01:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 10 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt4_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:03:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-769b7167-49e5-48c4-b0da-a4d929ef12d5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:03:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 8 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt4_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt5 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:05:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-247f3d54-27b7-4a11-88d0-763b1c5a4a48. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:05:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 8 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt5_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:08:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-35327d98-a0c9-4ce4-8db7-927cf21adae1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:08:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 4 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt5_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:10:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7ca6f758-1799-41af-bac5-3fa45acf391c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:10:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 1 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt5_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:12:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b68b15c9-03d4-49db-80e1-cd3535dc2e7c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:12:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 22 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt5_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt6 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:14:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-566a722e-7d9e-480f-bd8e-7ad57c1e4044. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:14:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 18 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt6_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:17:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f80bd97f-6410-4241-9c3a-0723a1c2b1c9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:17:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 17 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt6_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:19:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ef37e735-7d64-443a-8bba-f90bf32b04ad. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:19:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 12 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt6_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:21:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bf3e5079-97b7-4852-a24f-6838b3e15b74. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:21:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 11 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt6_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt7 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:23:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d916fae2-8c8d-4ab7-8b53-fea13cd396af. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:23:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 7 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt7_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:25:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c88dba4a-14e6-45f6-8398-d63edf9ba2b2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:25:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 6 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt7_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:28:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ce27e39b-4f2c-4730-b611-71d28b0d1684. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:28:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 2 due to category error: Error: Subcategory I.2.7 was given, which is not an acceptable subcategory.\n",
      "Saved llm_prompt7_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:30:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-aed7f63b-e729-49a4-858b-f38c18eacf5a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:30:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llm_prompt7_gemini-2_0-flash_iter4.csv\n",
      "\n",
      "Running llm_prompt8 with gemini-2.0-flash\n",
      "  ▶ Iteration 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:32:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-92de04a0-632e-49b4-8b02-9719e979a19a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:32:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llm_prompt8_gemini-2_0-flash_iter1.csv\n",
      "  ▶ Iteration 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:34:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e102614e-baa1-46fc-a566-902dc66f0821. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:34:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llm_prompt8_gemini-2_0-flash_iter2.csv\n",
      "  ▶ Iteration 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:37:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dea9fdf1-4875-4575-bc03-fe0e33b97c84. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:37:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llm_prompt8_gemini-2_0-flash_iter3.csv\n",
      "  ▶ Iteration 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 16:39:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2c1c7fae-293c-47bb-9d32-6e47610947ac. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/23 16:39:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved llm_prompt8_gemini-2_0-flash_iter4.csv\n",
      "Summary saved as prompt_model_mean_errors.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm_prompt</th>\n",
       "      <th>mean_error_gemini-2_0-flash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llm_prompt1</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llm_prompt2</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llm_prompt3</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llm_prompt4</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llm_prompt5</td>\n",
       "      <td>0.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llm_prompt6</td>\n",
       "      <td>0.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llm_prompt7</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llm_prompt8</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    llm_prompt  mean_error_gemini-2_0-flash\n",
       "0  llm_prompt1                        0.799\n",
       "1  llm_prompt2                        0.380\n",
       "2  llm_prompt3                        0.302\n",
       "3  llm_prompt4                        0.188\n",
       "4  llm_prompt5                        0.432\n",
       "5  llm_prompt6                        0.651\n",
       "6  llm_prompt7                        0.604\n",
       "7  llm_prompt8                        0.535"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"gemini-2.0-flash\"]\n",
    "prompts = [f\"../assets/llm_prompt{i}\" for i in range(1, 9)]\n",
    "NUM_ITERATIONS = 4\n",
    "MAX_REQUESTS = 25\n",
    "error_summary = []\n",
    "\n",
    "for prompt_file in prompts:\n",
    "    for model_name in models:\n",
    "        print(f\"\\nRunning {prompt_file} with {model_name}\")\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        iteration_errors = []\n",
    "\n",
    "        for iteration in range(1, NUM_ITERATIONS + 1):\n",
    "            print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "            received_rows = []\n",
    "\n",
    "            # === Fetch 30 JSONs from stream ===\n",
    "            def handle_batch(df, batch_id):\n",
    "                global received_rows\n",
    "                pdf = df.toPandas()\n",
    "                for _, r in pdf.iterrows():\n",
    "                    received_rows.append(r.to_dict())\n",
    "                    if len(received_rows) >= MAX_REQUESTS:\n",
    "                        threading.Thread(target=query.stop).start()\n",
    "\n",
    "            query = json_stream_df.writeStream.foreachBatch(handle_batch).start()\n",
    "            query.awaitTermination()\n",
    "\n",
    "            if len(received_rows) < MAX_REQUESTS:\n",
    "                print(\"Skipping this iteration (not enough samples)\")\n",
    "                continue\n",
    "\n",
    "            # === Prediction and Scoring ===\n",
    "            eval_results = []\n",
    "            with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                base_prompt = f.read()\n",
    "\n",
    "            for article in received_rows[:MAX_REQUESTS]:\n",
    "                json_str = json.dumps(article, separators=(\",\", \":\"))\n",
    "                full_prompt = f\"Sample to predict:\\n\\n{json_str}\\n\\n{base_prompt}\"\n",
    "\n",
    "                try:\n",
    "                    response = model.generate_content(full_prompt)\n",
    "                    raw = response.text.strip()\n",
    "                    if raw.startswith(\"```json\") or raw.startswith(\"```\"):\n",
    "                        raw = raw.strip(\"`\").replace(\"json\", \"\", 1).strip()\n",
    "                    pred = json.loads(raw)\n",
    "                except Exception:\n",
    "                    pred = {\"main_category\": \"unknown\", \"categories\": \"unknown\"}\n",
    "\n",
    "                eval_results.append({\n",
    "                    \"Aid\": article.get(\"aid\"),\n",
    "                    \"Title\": article.get(\"title\"),\n",
    "                    \"Main Category\": pred.get(\"main_category\", \"N/A\"),\n",
    "                    \"Categories\": pred.get(\"categories\", \"N/A\"),\n",
    "                    \"True Main Category\": article.get(\"main_category\", \"N/A\"),\n",
    "                    \"True Categories\": article.get(\"categories\", \"N/A\")\n",
    "                })\n",
    "                time.sleep(4.1)\n",
    "\n",
    "            df_temp = pd.DataFrame(eval_results)\n",
    "\n",
    "            # Error handling\n",
    "            errors = []\n",
    "            for idx, row in df_temp.iterrows():\n",
    "                try:\n",
    "                    pred = {\n",
    "                        \"main_category\": row[\"Main Category\"],\n",
    "                        \"categories\": row[\"Categories\"]\n",
    "                    }\n",
    "                    true = {\n",
    "                        \"main_category\": row[\"True Main Category\"],\n",
    "                        \"categories\": row[\"True Categories\"]\n",
    "                    }\n",
    "                    err = pred_vs_pred_error(pred, true)\n",
    "                except AssertionError as e:\n",
    "                    print(f\"Skipping row {idx} due to category error: {e}\")\n",
    "                    err = None\n",
    "                errors.append(err)\n",
    "\n",
    "            df_temp[\"Error\"] = errors\n",
    "            df_temp_clean = df_temp.dropna(subset=[\"Error\"])\n",
    "\n",
    "            # Save per-iteration detailed results\n",
    "            fname = f\"../outputs/{prompt_file}_{model_name.replace('.', '_')}_iter{iteration}.csv\"\n",
    "            df_temp.to_csv(fname, index=False)\n",
    "            print(f\"Saved {fname}\")\n",
    "\n",
    "            if not df_temp_clean.empty:\n",
    "                iteration_errors.append(df_temp_clean[\"Error\"].mean())\n",
    "\n",
    "        # Store final mean error for this prompt-model combo\n",
    "        mean_error = round(sum(iteration_errors) / len(iteration_errors), 3) if iteration_errors else None\n",
    "        error_summary.append({\n",
    "            \"llm_prompt\": prompt_file,\n",
    "            f\"mean_error_{model_name.replace('.', '_')}\": mean_error\n",
    "        })\n",
    "\n",
    "# === Final summary ===\n",
    "df_summary = pd.DataFrame(error_summary)\n",
    "df_summary.to_csv(\"../outputs/prompt_model_mean_errors.csv\", index=False)\n",
    "print(\"Summary saved as prompt_model_mean_errors.csv\")\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70e2d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef658b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_analytics_bsns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
